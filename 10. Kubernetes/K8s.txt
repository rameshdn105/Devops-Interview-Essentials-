Q. What is monolithic appn and microservices and why are we moving from monolithic to microservices?
-> Monolithic application: Single, unified unit while a microservices architecture is a collection of smaller, independently deployable services.
-> Because you can detect bugs or performance issues in a gradual fashion.
-> In microservices small services can be written in different languages(Python, Node.js, Java) but where as in monolithic its more of a single language.
-> Microservice would be cost effective as it can manipulate servers depending on the traffic, but in monolithic it has to create a lot number 
   of services as per traffic.
-> Ex for microservice: Kubernetes.

		Monolithic					Microservices
modules, together deployed with tight coupling		deploy k8s, modules will be individually 
through ansible, automation Jenkins			 
downtime high						no downtime, i.e, downtime other application is not possible

-> Container orchestration is the automation of much of the operational effort required to run containerized workloads and services.
   This includes a wide range of things software teams need to manage a container's lifecycle, including provisioning, deployment, scaling (up and down),
   networking, load balancing and more

=====================================================================================================================================
Definition of Docker Swarm:
-> The Docker Swarm is essentially a kind of tool which allows us to create and schedule the multiple docker nodes easily. 
-> The docker swarm can also be used for a vast number of docker nodes. Each Node in the docker swarm is itself actually a docker daemon, and that demon is 
   able to interact with the Docker API and has the benefits of being a full docker environment.
-> Docker Swarm can reschedule containers on node failures. 
-> Swarm node has a backup folder which we can use to restore the data onto a new Swarm.

# Features of Docker Swarm
1. Decentralized access: Swarm makes it very easy for teams to access and manage the environment 
2. High security: Any communication between the manager and client nodes within the Swarm is highly secure 
3. Autoload balancing: There is autoload balancing within your environment, and you can script that into how you write out and structure the Swarm environment allowing you to increase or decrease the number of replicas of a service as desired. 
4. High scalability: Load balancing converts the Swarm environment into a highly scalable infrastructure
5. Rolling updates: Docker Swarm enables easy rolling updates.

#  What are Docker Swarm Nodes? 
-> A docker swarm can recognize three different types of nodes, and each type of Node plays a different role within the ecosystem of the docker swarm.
#  Types of Nodes: Leader Node, Manager and Worker node

=====================================================================================================================================
Kubernetes vs. Docker Swarm

   Features				Kubernetes							Docker Swarm
-Installation&clusture		Complex, clustre is strong					Simple, clusture is not strong
-GUI				GUI is K8s dasboard						There is not GUI
-Autoscaling			It can do auto scaling						Can do autoscaling, but slower
-Scalability			Scaling and deployment are comparatively slower 		Containers are deployed much faster
-Logging & monitoring		In-built tool logging & monitoring				3rd part tools like ELK (Elasticsearch, Logstash, Kibana)
-Rolling updates & Rollback	Both it can do							Can only do Rolling updates
-Load Balancing			Manual intervention is required	for load balancing		Automated load balancing				 
-Container Setup		Commands like YAML should be rewritten 				A container can be easily deployed to different platforms
					while switching platforms
-Availability			High availability when pods are distributed 			Increases availability of applications through redundancy
					among the nodes
-Data volumes			Shared with containers from the same pod			Can be shared with any container

-> Docker has its own tool called Docker Swarm. Kubernetes from Google and Mesos from Apache. 
-> While Docker Swarm is really easy to setup and get started, it lacks some of the advanced autoscaling features required for complex applications. 
-> Mesos on the other hand is quite difficult to setup and get started, but supports many advanced features. 
-> Kubernetes - arguably the most popular of it all – is a bit difficult to setup and get started but provides a lot of options to customize deployments and supports deployment of complex architectures
=====================================================================================================================================
Kubernetes Overview: {K8S-Container orchestration tool}
-> Kubernetes, is an open-source system for automating deployment, scaling, and management of containerized applications. 
-> K8s, is written primarily in the "Go" open source programming language or "Golang". It is known for its simplicity, speed, and efficient memory management. 
-> It groups containers that make up an application into logical units for easy management and discovery.
-> Daemon service: 100% of service is available.
-> Monitor K8S: -> Prometheus, Grafana, Fludentd, Elasticsearch, Kiabana, Stackdriver, New Relic, Datadog.

Kubernetes Features:
--------------------
1. Self-healing: Restarts containers that fail, replaces and reschedules containers when nodes die, kills containers that don't respond to your user-     defined health check, and doesn't advertise them to clients until they are ready to serve. (Probe)

2. Automated rollouts and rollbacks: Kubernetes progressively rolls out changes to your application or its configuration, while monitoring application health to ensure it doesn't kill all your instances at the same time. 
-> If something goes wrong, Kubernetes will rollback the change for you. Take advantage of a growing ecosystem of deployment solutions.

3. Horizontal scaling: Scale your application up and down with a simple command, with a UI, or automatically based on CPU usage.
-> Horizontal scaling means that the response to increased load, is to deploy more Pods.  
-> Vertical scaling : Increasing CPU usage/Hardware capacities.

4. Service discovery and load balancing: No need to modify your application to use an unfamiliar service discovery mechanism. Kubernetes gives Pods their own IP addresses and a single DNS name for a set of Pods, and can load-balance across them.

--------------------------------------------------------------------------------------------------------------------------------
** kubectl: Command line interface, it is tool to connect between api server and user. (kubectl run, cluster-info, get nodes)
	$$ kubectl run hello-minikube
	$$ kubectl cluster-info
	$$ kubectl get nodes

** Kubeadm is a tool used to build Kubernetes (K8s) clusters. Kubeadm performs the actions necessary to get a minimum viable cluster up and running quickly.
	$$ kubeadm validate cluster 

** kubeconfig: It is a configuration file used by kubectl, to connect to a Kubernetes cluster. (Easily switch between different clusters and users in the same configuration file)
-> The kubeconfig file contains information such as the location of the API server, the credentials used  to authenticate with the API server,
 and the current context (the cluster, user, and namespace that kubectl is currently interacting with).
-> Usually located in the user's home directory under ~/.kube/config but specified with "KUBECONFIG" environment variable or the --kubeconfig flag.

=====================================================================================================================================
Kubernetes Architecture and Components:

Kubernetes Master components:
DEVOLOPERS===>API Server--->(Key value store-etcd :: controllers :: Scheduler)====> Master node
											|
	     Kubelet---->Container runtime----> Network proxy---->Pod1, Pod2...====> Worker node
											|
										       USERS
Kubernetes Master Node: 
CLI/UI-->API-->(API server::SCHEDULER::CONTROLLERS::KEY VALUE STORE-etcd)

-> Nodes(Minions): A node is a machine – physical or virtual – on which Kubernetes is installed. 
-> A node is a worker machine and this is were containers will be launched by Kubernetes
-> A cluster is a set of nodes grouped together. This way even if one node fails you have your application still accessible from the other nodes. Moreover having multiple nodes helps in sharing load as well.

1. Master Node:
===============
-> The Kubernetes Master (Master Node) receives input from a CLI (Command-Line Interface) or UI (User Interface) via an API. 
-> Through CLI you define pods, replica sets, and services that you want Kubernetes to maintain. 
	For example, which container image to use, which ports to expose, and how many pod replicas to run.
-> You also provide the parameters of the desired state for the application(s) running in that cluster.

 
1. API server: ( Front-end for k8s, Enables the communication)
-> The users, management devices, Command line interfaces all talk to the "API server" to interact with k8s cluster.
-> The API Server is the front-end of the control plane and the only component in the control plane that we interact with directly.
-> All the components in the k8s cluster communicates through API server, no other component can communicate with each other directly.
-> API server is also responsible for "authentication and authorization".
-> API server has got the watch mechanism to watch the changes in the worker nodes.
-> API server is the "heart" of the k8s cluster most of the decision are done by API server

--> what process used to run k8s master node?   ans: API server.
	
	   	
2. Key-Value Store (etcd):  (It stores the current information)
-> By default, etcd data is stored in folder /var/lib/etcd , which is most likely stored on the root file system.
-> etcd is a distributed, consistent key-value store which stores the configuration data of the complete Kubernetes cluster.
-> Configuration data represent the desired state of the cluster Ex:-
	- which nodes exist in the cluster
	- what are the pods running, and on which node they are ruuning on 
	- whole lot information on the cluster.
	- To backup the cluster we need to backup the etcd
-> Secure: (Authentication, Encryption, Authorization, Network segmentation, Backup and restore, Monitor and Audit, Update and patch)
-> ETCD is responsible for implementing locks within the cluster to ensure there are no conflicts between the Masters.
-> When it comes to locks within the cluster, ETCD ensures that only one master (or component) performs an action at a time to avoid conflicts.


3. Scheduler: (Decides where our container will be running on which node)
-> The scheduler is the one which decides which pod should be created on which worker node.
-> Responsible for distributing work or containers across multiple nodes.
-> Always watches the unscheduled pods/ new pods and binds them to a worker node based on the availability of the requested resources, service requirements, affinity  and anti-affinity specifications and other constraints.

				  
4. Controller manager: (Responsible for monitoring worker node, your containers and authentication, authorization.)
-> It runs a set of controllers that are responsible for ensuring that the desired state of the cluster matches the actual state. 
-> Essentially, it watches the cluster state, makes decisions to maintain the desired state, and ensures that Kubernetes components and resources are running as intended.
-> Controller manager is a daemon which always runs the core control loops known as controllers. 
-> Controller watches the state of the cluster through the API server watch feature.
-> When it get notified, it makes to move current state towards the desired state.
-> The container runtime is the underlying software that is used to run containers. In our case it happens to be "Docker".


2. Worker Node Components:
==========================

1. kubelet: (It does heavy lifting on container, fetch image, map volumes, run containers on the nodes as expected)
-> It is the main agent that run on each worker node and communicates with API server to apply the desired state to the node.
-> It sends all the metrics of the worker node to API server using a tool called cAdvisor.
-> It communicates with the docker daemon using docker socket api to create pods and to change the configurations of pods.

The main responsibilities of kubelet are:
	- Run/create the pod with container runtime.
	- Always reports the status of pods to API server using a tool called cAdvisor.
	- Reports the current status of worker node and pods to API server

-> cAdvisor is an open-source agent integrated into the kubelet binary that monitors resource usage and analyzes the performance of containers. 
-> It collects statistics about the CPU, memory, file, and network usage for all containers running on a given node. (it does not operate at the pod level).

			
2. kube-proxy / Service Proxy: (expose pod to external world we can use kube-proxy, or we can set network rules)[Service Discovery, Load Balancing, Network traffic forwarding, Node port services]
-> kube-proxy is a network proxy that runs on each node in your cluster, implementing part of the Kubernetes Service concept.
-> It ensures that network traffic is properly routed to the correct pod within a cluster.
-> It implements Kubernetes service discovery and load balancing at the node level.  
-> This monitors the assignment of IP address to pods.
-> Entire network configuration is maintained by service proxy.	

Modes of Operation: kube-proxy supports different modes to manage traffic routing:
1. iptables Mode (Default): 
-- Uses iptables rules to forward traffic to the correct pod 
-- Scales well because packets are handled at the kernel level
-- Load balances traffic across pods.
2. IPVS Mode (Recommended for Large Clusters)
-- Uses IP Virtual Server (IPVS) for more efficient load balancing.
-- Supports advanced scheduling algorithms (e.g., round-robin, least connections).
-- Better performance than iptables for high-traffic workloads.
3. Userspace Mode (Legacy, Not Recommended)
-- Uses a user-space proxy to forward traffic.
-- Slower than iptables and IPVS because traffic goes through an additional hop.

			
3. Container runtime: 
-> Software that is responsible for running containers.
-> containerd (default): Lightweight, designed for Kubernetes, used by Docker & CRI-O.
-> Kubernetes does not run containers directly—instead, it delegates container execution to a Container Runtime Interface (CRI)-compliant runtime.
-> Kubernetes support several container runtime: Docker, containerD, CRI-O (Container Runtime Interface for OpenShift)
-> In our company we are using Docker as containerization technology.
	
** What is a Container Runtime?
-> A container runtime is responsible for:
✅ Pulling container images from a registry (e.g., Docker Hub, ECR, GCR).
✅ Creating and managing containers.
✅ Handling networking and storage for containers.
✅ Enforcing resource limits (CPU, memory).
	
=====================================================================================================================================
** Alternatives:

-> Amazon ECS(Elastic container service), Docker enterprise, Azure Kubernetes service, Google Kubernetes Engine (GKE), Redhat, Portainer, Saltstack, Rancher
-> Problem statement- running multiple containers, scaling, managing many containers, when they fail to run, support and communicate
* Available tools
1. Kubernetes
2. docker swarm
3. Apache mesos

** Methods of Setting up/installing Kubernetes, Types of cluster available in market, Types of k8s cluster
1. MiniKube: Easy to run a single-node Kubernetes cluster locally on your machine.
-> Minikube provides an executable command line utility that will AUTOMATICALLY download the ISO and deploy it in a virtualization platform such as Oracle Virtualbox or Vmware fusion. So you must have a Hypervisor installed on your system. For windows you could use Virtualbox or Hyper-V and for Linux use Virtualbox or KVM. So you must have a hypervisor installed, kubectl installed and minikube executable installed on your system

2. kubeadmin: It is a tool used to configure kubernetes in a multi-node setup. (both for experimental production grade cluster)can also be used for production)

3. HArd Way
4.Managed K8s (Amazon Elastic Kubernetes Service, Azure Kubernetes Service, Oracle Kubernetes Service, Google Kubernetes Engine)

Q. How to setup kubernetes locally?
-> Minikube: Easy to run a "single-node Kubernetes cluster" locally on your machine. ($$ minikube start)(Windows, Linux, and macOS)


2. Docker for Desktop: Docker installed on your machine, you can use Docker for Desktop to set up a local Kubernetes cluster. (kubectl commands to interact with the cluster)
3. Microk8s: is a lightweight, fast, and simple Kubernetes distribution that runs natively on Linux. Run a local cluster on your machine with the command microk8s start.   
4. k3s: k3s is a lightweight Kubernetes distribution that is designed for resource-constrained environments.
5. kind (Kubernetes in Docker): It is a tool for running local Kubernetes clusters using Docker container "nodes". (create a cluster with a single command kind create cluster)
 
=====================================================================================================================================
Q. How to monitor k8s cluster?
-> Using built-in tools: kubectl, kubectl top, and kubectl logs (provide information about resource usage, pod status, and logs)
-> Using Prometheus and Grafana: Prometheus is an open-source monitoring and alerting system, while Grafana is an open-source visualization tool.
-> Commercial monitoring tools: Datadog, New Relic, and AppDynamics, provide advanced monitoring and visualization capabilities for Kubernetes clusters.
-> Using log aggregation and analysis tools: Elasticsearch, Logstash, and Kibana (ELK), can be used to collect and analyze logs from the cluster, providing a centralized view of log data.

=====================================================================================================================================
Container Runtimes:
1. containerd is an industry-standard container runtime that is designed to be lightweight and modular, it is used as the default runtime in some Kubernetes distributions like Rancher and OpenShift.
2. CRI-O is a Kubernetes-specific container runtime that is optimized for Kubernetes, it can work with both containerd and runc.
3. rkt is an alternative container runtime that is designed to be simple, secure and composable.
4. K8s: Docker and K8s tightly coupled.
** docker engine: where containers are running.

=====================================================================================================================================

POD:  [smallest object, ephemeral(self-healing), single/multi container]
---- 
-> pods are the smallest deployable objects that you can create in K8s.
-> pods are ephemeral (if pod fails to work Kubernetes will produce a replica of that pod: Self healing nature) by nature.
-> pods should contain at least one container and may contain many containers.
-> The containers are encapsulated into a Kubernetes object known as PODs.
-> Helper container, that might be doing some kind of supporting task for our web application such as processing a user entered data, processing a file uploaded by the user etc. 
-> The two containers can also communicate with each other directly by referring to each other as ‘localhost’ since they share the same network namespace. Plus they can easily share the same storage space as well. 

## Sidecar container/Multi container: This pattern is useful for scenarios where you need to add features such as monitoring, logging, proxying, or managing secrets without modifying the main application code.
1. Logging and Monitoring with a Sidecar
-> we will use the Fluentd sidecar container to collect logs from the primary application container and send them to an external logging system (like Elasticsearch or Splunk). This approach ensures that logs are collected without modifying the primary application.

2. Multi-Container Pod with Sidecar for Proxying (e.g., with Envoy)
-> A sidecar container is used to act as a proxy or API gateway for the primary application. Envoy, a popular proxy, can be used as a sidecar to manage network traffic, handle retries, timeouts, circuit breaking, and more.

3. Multi-Container Pod for Application with Caching Sidecar (e.g., Redis)
-> Use a sidecar to deploy a Redis container within the same pod. This sidecar could provide caching services to the application without needing to manage Redis as a separate service.

Commands:
-- To create pod: $$ kubectl run nginx --image=nginx 
-- To list pods: $$ kubectl get pods
-- To describe pod/check containers: $$ kubectl describe pod webapp
-- To delete pod: $$ kubectl delete pod  webapp
	-> k8s should automatically create pods to maintain the replicas
-- to create  containers from yaml: $$ kubectl apply -f example.yaml
-- to create deployment with 2 replicas: 
	$$ kubectl create deployment nginx-app --image=nginx --replicas=2
-- to expose port 80: $$ kubectl expose deployment nginx-app --type=NodePort --port=80

-- $$ kubectl run redis --image=redis123 --dry-run=client -o yaml > redis-definition.yaml
-- To list deployments: $$ kubectl get deployment

-> "--image" mentioned is downloaded from docker hub repository.

pod-definition.yaml
apiVersion:v1
kind: Pod
metadata:
  name: nginx
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx

# Connect to Container in a POD
kubectl exec -it <pod-name> -- /bin/bash

#To Get complete pod definition YAML output
kubectl get pod <podname> -o yaml 

#to get IP of all Pods
kubectl get pods -o wide

***********************************************************************************************
Lifecycle of pod phases / status:
1. pending 
-> It will wait for Kubernetes cluster to accept.
-> It will wait till all the containers in pod to be running.
-> pod will spend some time waiting to be scheduled to a node and in downloading container images over the network.
	
2. Running 
-> The pod has been bound to a node by scheduler.
-> All of the containers have been created.
-> At least one container is running or it may be in starting or restarting.
	
3. Succeeded
-> All the containers in the pod have been terminated successfully.
-> No container will be restarted.
	
4. Failed 
-> All the containers in the pod are not be running and any one container have been terminated in failure.
		
5. Unknown
-> For some reason the state of the pod could not be identified or obtained.
-> This status may also occur if Kubernetes cannot communicate with pod or node.
	
6. Terminating 
-> when a pod is being deleted.
-> This status is not one of the pod phases.

=====================================================================================================================================
YAML:
-----
-> Kubernetes uses YAML files as input for the creation of objects such as PODs, Replicas, Deployments, Services etc. 

-> Contains 4 top level fields:

1. apiVersion: 
-> (String) v1 / apps/v1 / apps/v1beta1 / extensions/v1beta1
-> Kubernetes provides stable, alpha and beta versions of api

2. kind: 
-> (String) type of object (Pod, Replica Set, Deployment, Service)
-> Object name first letter should be in uppercase.

3. metadata: 
-> It is data about the object like its name, labels etc. so that it can be uniquely identified by other objects
-> In the form of a dictionary.
-> Under metadata, the name is a string value, labels is a dictionary within the metadata dictionary
-> It can have any key and value pairs

4. spec: 
-> Specify the configuration to define the desired state of the object.
-> Spec is a dictionary so add a property under it called containers, which is a list or an array.

Resources:
-> https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.9/#replicaset-v1-apps
-> https://plugins.jetbrains.com/plugin/9354-kubernetes-and-openshift-resource-support

kind			version
POD			v1
Service			v1
ReplicationController	v1
ReplicaSet		apps/v1
Deployment		apps/v1
=====================================================================================================================================
Q. What happens when I create a pod/ Workflow of Kubuernates ?
1. kubectl writes to the API Server.
2. API Server validates the request and persists it to etcd.
3. etcd notifies back the API Server.
4. API Server invokes the Scheduler.
5. Scheduler decides where to run the pod on and return that to the API Server.
6. API Server persists it to etcd.
7. etcd notifies back the API Server.
8. API Server invokes the Kubelet in the corresponding node.
9. Kubelet talks to the Docker daemon using the API over the Docker socket to create container.
10. Kubelet updates the pod status to the API Server.
11. API Server persists the new state in etcd.

=====================================================================================================================================
1. Replication controller: [High availability, Load balancing & Scaling]
----------------------

-> Controllers are the brain behind Kubernetes.
-> They are processes that monitor Kubernetes objects and respond accordingly.
-> High availability: Replication controller maintenance pods to help not to fail application
-> Load Balancing & Scaling: Shares load across pods

Replica Set vs Replication Controller: 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~	
  
   - Replicaset apiVersion is "apps/v1" and For replication controller is "v1".
   - Replica set requires a "selector definition" and match labels under it.
   - Both are used to create defined replicas of pod at a given point of time.
   - These objects can be used individually.
   - Deployments always uses Replica Set.
   - Replica Set selects resources with set-based selectors.
   - Replication Controller selects resources with equality based selectors
	- To check Replication Controller used 
			kubectl get rc
	- To check Replica Set used 
			kubectl get rs


Q. So what is a replica and why do we need a replication controller? What if for some reason, our application crashes and the POD fails?
-> High availability: Users will no longer be able to access our application. To prevent users from losing access to our application, we would like to have more than one instance or POD running at the same time.
-> Another reason we need replication controller is to create multiple PODs to share the load across them.

-------------------------------------------
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end

spec:
  template: 
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx

  replicas: 3


# kubectl create -f rc-definition.yaml
# kubectl get replicationcontroller
# kubectl get pods

===================================================================================================
2. Replica sets: [High availability, Load balancing & Scaling]
-----------------
-> It is very similar to replication controller.
-> The apiVersion though is a bit different. It is "apps/v1"
-> If you provide "v1" instead of "apps/v1" you would get "no match for kind replicaset", because the specified Kubernetes api version has no support for ReplicaSet.
-> one major difference between replication controller and replica set. Replica set requires a "selector definition". The selector section helps the replicaset identify what pods fall under it.

Q. But why would you have to specify what PODs fall under it, if you have provided the contents of the pod-definition file itself in the template?
-> It’s because, replica set can also manage pods that were not created as part of the replicaset creation.
-> Say for example, there were pods created before the creation of the ReplicaSet that match the labels specified in the selector, the replica set will also take those pods into consideration when creating the replicas.
-> The match Labels selector simply matches the labels specified under it to the labels on the PODs.

apiVersion: app/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end
spec:
  template: 
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx
        
  replicas: 3
  selector: 
    matchLabels:
      type: front-end


$$ kubectl create -f replicaset-definition.yaml
$$ kubectl replace -f replicaset-definition.yaml (to replace updates in manifest file)
$$ kubectl get replicaset
$$ kubectl delete replicaset myapp-replicaset
$$ kubectl get pods

$$ kubectl scale --replicas=6 -f replicaset-definition.yaml
$$ kubectl scale --replicas=6 replicaset myapp-replicaset

===================================================================================================
LABELS & SELECTORS:
-------------------
Q. Why do we label our PODs and objects in Kubernetes?
-> Say we deployed 3 instances of our frontend web application as 3 PODs. We would like to create a replication controller or replica set to ensure that we have 3 active PODs at anytime. And YES that is one of the use cases of replica sets. You CAN use it to monitor existing pods, if you have them already created, as it IS in this example. In case they were not created, the replica set will create them for you. The role of the replicaset is to monitor the pods and if any of them were to fail, deploy new ones. The replica set is in FACT a process that monitors the pods. Now, how does the replicaset KNOW what pods to monitor. There could be 100s of other PODs in the cluster running different application. This is where labelling our PODs during creation comes in handy. We could now provide these labels as a filter for replicaset. Under the selector section we use the match Labels filter and provide the same label that we used while creating the pods. 
This way the replicaset knows which pods to monitor.

Q. We started with 3 replicas and in the future we decide to scale to 6. How do we update our replicaset to scale to 6 replicas?
->  The first, is to update the number of replicas in the definition file to 6. Then run the $$ kubectl replace command specifying the same file using the –f parameter and that will update the replicaset to have 6 replicas.
-> The second way to do it is to run the kubectl scale command. Use the replicas parameter to provide the new number of replicas and specify the same file as input.

** Labels of pod and Replica set labels should match.
** ReplicaSet won't allow to create more replica of pods using command with same label and different name.

Selector: To identify what pod falls under it, 
Q. when we define pod definition in yaml file, then why do we need to define selector? 
- it also manages the pods which is not created by it.

kubectl create -f replicaset-definition.yaml
kubectl get replicaset
kubectl delete replicaset myapp-replicaset
kubectl replace -f replicaset-definition.yml
kubectl scale –replicas=6 -f replicaset-definition.yml
kubectl get pods

To increase replicas from "3" to "6"
Update manifest file of replicaset and run : kubectl replace -f replicaset-definition.yaml

or 

$$ kubectl scale --replicas=6 -f replicaset-definition.yaml
or
$$kubectl scale --replicas=6 replicaset myapp-replicaset


** To edit running configuration of replicaset:
$$ kubectl edit -f replicaset myapp-replicaset

$$ kubectl scale replicaset myapp-replicaset --replicas=2

===================================================================================================
Labels:
   - k8s labels are applied to objects which allow to identify, select and operate on objects with label applied.
   - labels are key value pairs that can be applied/attached to pods, services, deployment, DaemonSet, nodes etc.
   - keys are defined by kubernetes and it is not user-defined.
   - Label key should not contain special character.
   - Values can include dot, but start and ending can only be alpha numeric.
	
	To list the labels of a pod 
		sudo kubectl get pod <pod_name> --show-labels 

	To list the labels of a object 
		sudo kubectl get object <object_name> --show-labels
		
	Add a label to a pod 
		sudo kubectl label pod <pod_name> <label_key> <label_value>	(Using YML also we can label)
		
	Add a label to a node
		sudo kubectl label node <node_name> <label_key> <label_value>
		
	To give multiple labels
			- app: nginx-deployment
			- tier: frontend
				
	Diff between labels and metadata
		labels are user-defined and metadata are pre-defined
		
=======================================================================================================================================
Selectors:
	- selectors help us to identify/filter out the objects using matching labels of the objects.
	
	*Equality Based selector 
	   	- comparison is based on only equality or inequality.
		- Three kinds of operators that I can use is = or ==, != 
		- Can only check single set of values.
			ex: app = front-end or app == front-end 
				environment != prod
        kubectl get pods -l environment=production,tier=frontend
				
	*Set-Based selector 
		- It allow us to filter resources according to set of values.
		- Three kind of operators in, notin and exists.
				ex: app in (front-end, back-end)
       kubectl get pods -l 'environment in (production),tier in (frontend)'	(-l:labels)

=======================================================================================================================================
Kubernetes configuration / desired state 
apiVersion: v1
kind: Deployment 
metadata:
  name: static-web
  labels:
	role: myrole
spec:
  containers:
	- name: web
	  image: nginx
	  ports:
		- name: web
		  containerPort: 80
		  protocol: TCP

To create an object from a kubernetes spec file 
	sudo kubectl create -f <spec_yaml_path>

To apply the changes of spec file to the object 
	sudo kubectl apply -f <spec_yaml_path>
	
To list the objects in kubernetes 
	sudo kubectl get <object_type>

To delete an object 
    sudo kubectl delete <object_type> <object_name>

To check the details of object 
	sudo kubectl describe <object_type> <object_name>	
	
=======================================================================================================================================
Default Controllers of Kubernetes:
-> When we install Kubernetes we will get some controllers by default and It is used by Kubernetes only.
-> The Controller Manager runs several controllers, each with a specific job. 

** How the Controller Manager Works:
------------------------------------
1. Watchers: The Controller Manager watches for changes in the cluster's state through the Kubernetes API server. When a change occurs (such as the creation of a Deployment), the relevant controller inside the Controller Manager takes action.

2. Reconciliation Loop: For example, in the case of a ReplicaSet controller, if the actual number of replicas is less than desired (perhaps due to a pod failure), the ReplicaSet controller will notice this and create new Pods to meet the desired number of replicas.

3. Event-driven: The controllers act in response to events. When something changes (such as a Pod is deleted or a new Deployment is created), the controller reacts by making the necessary changes to ensure the system is in the desired state.

Different type of Controllers:
1. Deployment
2. ReplicaSet
3. DaemonSet
4. StatefulSet
5. Job
6. CronJob

Here are a few of the key controllers:
--------------------------------------
1. Replication Controller (or ReplicaSet):
-> Ensures that a specified number of replicas of a Pod are running at any given time. If there are too few, it will create more Pods. If there are too many, it will terminate excess Pods.

2. Deployment Controller:
-> Manages the rollout and updates of Deployments. It makes sure that the desired number of Pods are running and that they are updated in a controlled manner when the deployment changes.

3. StatefulSet Controller:
-> Manages StatefulSets, which are like Deployments but designed for applications that require stable identities and persistent storage, like databases.

4. DaemonSet Controller:
-> Ensures that a particular Pod runs on every node in the cluster. Useful for things like logging agents or monitoring agents that need to run on each node.

5. Job Controller:
-> Manages Jobs in Kubernetes, ensuring that a specified number of Pods successfully complete a task. Jobs are often used for batch or one-time tasks.

6. CronJob Controller:
-> Manages CronJobs, which run jobs on a scheduled basis (similar to cron jobs in Linux). It ensures that the jobs are executed at the correct time intervals.

7. Endpoint Controller:
-> Updates the Endpoints objects, which represent the set of network endpoints (usually Pods) for a particular Service.
-> Populates the information of endpoint objects (pods, services, jobs, deployment, replicas ...)

8. Namespace Controller:
-> Ensures that namespaces are properly managed within the cluster, especially when namespaces are created or deleted.

9. ResourceQuota Controller:
-> Enforces resource quotas (like CPU or memory limits) for namespaces and ensures that they are not exceeded.

1. Node controller:
-> Looks for node status and responds to API server when a node is down

==================================================================================	  
** Service Discovery: 
-> There are 2 ways to discover a service 
a. DNS (This is recommended method,   DNS: Domain name server)
-> The DNS server is added to the cluster in order to watch the Kubernetes
-> API create DNS record sets for each new service.
b. ENV var 
-> pod runs on a node, so that the kubernetes adds environment variables for each active service (each pod).

=======================================================================================================================================
Rollout and Versioning:
-----------------------
-> A rollout is the process of gradually deploying or upgrading your application containers. When you first create a deployment, it triggers a "rollout". A new rollout creates a new Deployment "revision". 

$$ kubectl rollout status deployment/myapp-deployment
$$ kubectl rollout history deployment/myapp-deployment


Deployment Strategy:
-------------------

1. Recreate strategy: Old deployment will be deleted and create again
-> One way to upgrade these to a newer version is to destroy all of these and then create newer versions of application instances. 
-> Meaning first, destroy the 5 running instances and then deploy 5 new instances of the new application version.
-> Problem: is that during the period after the older versions are down and before any newer version is up, the application is down and inaccessible to users. 

2. Rolling Update: (Default Deployment Strategy)
-> we take down the older version and bring up a newer version one by one. 
-> A Rolling Update in Kubernetes is a deployment strategy that allows you to update the application running in a cluster without downtime. 
-> It gradually replaces old versions of Pods with new ones, ensuring that there is no interruption in service. This is especially useful when deploying new versions of applications, as it minimizes the impact of the update.
-> When a Deployment is updated in Kubernetes, the Rolling Update strategy ensures that Pods are updated one at a time (or in batches, depending on the configuration). 
-> During the update, Kubernetes ensures that a certain number of replicas of the old version are still running, so the application remains available.

Key Concepts of Rolling Update:
1. A Deployment manages a ReplicaSet, which in turn manages the Pods.
2. By default, Kubernetes uses the RollingUpdate strategy for Deployments. This ensures a smooth and gradual update process with no downtime.
3. You can control how many Pods are added or removed during the update process using two parameters in the RollingUpdate strategy: maxSurge and maxUnavailable.
 
** maxSurge: Maximum number of Pods that can be created above the desired replicas
** maxUnavailable: Maximum number of Pods that can be unavailable during the update

Default Behavior:
-> maxSurge: The default value is 25% of the desired Pods (if not specified). For example, if you have 3 replicas, the default surge would be 1 (since 25% of 3 is 0.75, rounded up to 1).
-> maxUnavailable: The default value is also 25% of the desired Pods. For example, with 3 replicas, 25% would be 1.

----------------------------------------------------------------------------------------------------------------------------------------------
** Rollback the deployment:
-> Sometimes, you may want to rollback a Deployment; For example, when the Deployment is not stable, such as crash looping.
-> By default, all of the Deployment's rollout history is kept in the system so that you can rollback anytime you want.
(you can change that by modifying revision history limit).
		  
	- To check rollout status
	$$ kubectl rollout status deployment/nginx-deployment
			
	- Check the old replicas
	$$ kubectl get rs
			
	$$ kubectl describe deployment (Command)
		
	# To rollout:
	- First, check the revisions of this Deployment:
	nginx-deployment
				
	- To see the details of each revision, run:
$$ kubectl rollout history deployment.v1.apps/nginx-deployment --revision=2
				
	- Now you've decided to undo the current rollout and rollback to the previous revision:
$$ kubectl rollout undo deployment.v1.apps/nginx-deployment
				
	- Alternatively, you can rollback to a specific revision by specifying it with --to-revision:
$$ kubectl rollout undo deployment.v1.apps/nginx-deployment --to-revision=2

Q. how do you delete deployment?      
-> $$ kubectl delete deployment <deployment name>
   $$ kubectl delete object <object name>

=======================================================================================================================================
1. Deployment:
-> To create or modify instances of the pods that hold a containerized application.
-> Deployment can maintain multiple set of pods at a given point of time using ReplicaSet.
-> Deployment watches whether all the instances of pod is running or not, if not running deployment will create a new pod instance to maintain the number of replica using ReplicaSet.
-> Deployment makes Scaling of pods easy, by changing the number of pods we need at a given point of time.
-> We can easily expose a pod to the outside world means outside the cluster.
-> We can rollback to an earlier deployment version.
-> We can also manage the states of the pod paused, edited and rollbacked.
-> Rolling and rollback of updates to all pod instances using deployment is easy.
	
1. Scaling: 
-> we can change the value for number of replicas in spec file.
-> we can also use the below command 
$$ sudo kubectl scale deployment.v1.apps/nginx-deployment --replicas=3
	
2. Autoscalling (deployment internal autoscaller)
-> kubernetes can scale deployment automatically based on resource usage.
$$ sudo kubectl autoscale deployment.v1.apps/nginx-deployment --min=4 --max=20 --cpu-percent=80	

3. Vertical scaling : (Increasing CPU usage/Hardware capacities)
$$ kubectl set resources deployment <deployment-name> -n <namespace> --containers=<container-name> --requests=cpu=1000m,memory=512Mi --limits=cpu=2000m,memory=1Gi

*********************************************************************
# Create a new Deployment with the below attributes using your own deployment definition file.
# Name: httpd-frontend;
# Replicas: 3;
# Image: httpd:2.4-alpine

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: httpd-frontend
  labels:
    tier: front-end
    app: httpd
spec:
  replicas: 3
  selector:
    matchLabels:
      name: httpd-frontend
  template:
    metadata:
      labels:
        name: httpd-frontend
    spec:
      containers:
      - name: httpd-frontend
        image: httpd:2.4-alpine

=======================================================================================================================================
2. ReplicaSet 
	- A ReplicaSet is a set of multiple, identical pods with no unique identities. 
        - ReplicaSets were designed to address two requirements:	  
		•Containers are ephemeral. When they fail, we need their Pod to restart.
		•We need to run a defined number of Pods. If one is terminated or fails, we need new Pods to be activated.						
	- A ReplicaSet ensures that a specified number of Pod replicas are running at any given time.


3. DaemonSet:			
	- A DaemonSet ensures that a copy of a pod is always running on all the nodes in the cluster.
	- If a new node is added/removed from cluster then DaemonSet will automatically adds/deletes the pods from that node.	  
Usage: 
1) Monitoring agents: we need a agent called "Node exporter" to be running in every worker node to monitor all the nodes in cluster.
2) Logs collection Daemon: If we want to export logs of node and pods running in it we can create that "log exporter" using DaemonSet.				

Limitations:
1) it does not automatically run on any node which has taint.
Ex: Master, we need to specify the tolerations for it on the pod.

Monitoring tools: Prometheus, Grafana

==================================================================================
4. StatefulSet:
	- StatefullSets, just like deployment controller, creates the pod according to that manifest and replicas mentioned it in. 
	- StatefulSet gives the pods with a unique identity and well defined name for the pods to address each other.
	- StatefulSets maintains the sticky identity for each pod which will remain even if the pod gets killed and new pod replaces it. 
	  
    - It creates the Stateful application:
    - For the StatefulSets to work, a headless service should be created. 
    - Also StatefulSets need persistent data storage, so that the application saves the data and states across the restarts. 	 
    - StatefulSets are recommended when running Cassandra, MongoDB, MySQL, PostgreSQL or any other workload utilizing persistent storage. 
    - They can help maintain state during scaling and update events, and are particularly useful for maintaining high availability.	   


5. Jobs	
	- Jobs creates the pods to carry out short lived workloads which might be performing a single task. 
	- Once the task assigned to the pod completes it will shut down by itself.
-> Common use cases include processing background jobs like image processing, data analysis, ETL (Extract, Transform, Load), and database migrations.
	  
	- Few tasks these Job Pods do are:
		a.Running a migration activity
		b.Rotating logs
		c.One time initialization of resources.
			-In case any pod or node fails in between the task the job controller ensures new pod/node is  
			 created as replacement and resumes the activity from there.		
			-Jobs can also run multiple Pods in parallel, giving you some extra throughput.
	
		
6. CronJobs
	- CronJobs create the pods to run the jobs in user defined schedule.
	- The schedule can set using the Cron syntax and every time the requires the job to run a pod will be created and
	  after the job succeed the pod goes to shutdown state. 
	- Eg 
		- Taking back up of any application at a scheduled time everyday.
		- The CronJob controller also allows you to specify how many Jobs can run concurrently,
	          and how many succeeded or failed Jobs to keep around for logging and debugging
	
		
----------------------------------------------------------------------------------------------------------------------------------
Stateful Applications 
	- stateful applicaiton saves the user session data at the server side.
	- if server goes down it is difficult to transfer the session data to other server.
	- This type of applicaiton will not work, if we want to implement 
	  autoscalling for our applicaiton.

Stateless Applications
	- user session data is never save at the server side.
	- using a single authentication gateway or client token method to 
	  validate the users once for multiple microservices.

Feature				Deployment						StatefulSet
Use Case	Stateless applications (e.g., web servers, APIs)	Stateful applications (e.g., databases, message queues)
Pod Identity	Pods are interchangeable (identical)			Each pod has a unique, persistent identity
Pod Naming	Randomly assigned names (e.g., nginx-abc123)		Sequential, stable names (e.g., db-0, db-1, db-2)
Scaling		Adds/removes pods without order				Pods are added/removed in a controlled sequence
Storage		Uses ephemeral storage (deleted when pod terminates)	Uses Persistent Volume Claims (PVCs), which persist across restarts
Network Identity	No stable DNS name for each pod			Each pod gets a stable network identity (e.g., db-0.svc.cluster.local)
Rolling Updates		Supports rolling updates and rollbacks easily	Updates must be carefully managed (pods are updated one by one)

4. When to Use Which?
Scenario			Use Deployment?	   Use StatefulSet?
Stateless Web Applications	✅			❌
APIs or Microservices		✅			❌
Databases (MySQL, PostgreSQL)	❌			✅
Distributed Systems (Kafka, Zookeeper)	❌		✅
Cache (Redis, Memcached)	✅ (for stateless) / 	✅ StatefulSet (if persistence needed)	✅ (for master-slave setup)

===========================================================================================================================================================
Pod patterns / Container types 	
1. Init containers: 
	- Init containers are the containers that should run and complete
	  before the startup of the main container. (application container)
	- It provides a sperate lifecycle at initialization point.
	- can have multiple init containers. init containers always run to completion. 
          each init container must complete successfully before the next one starts.
When to use this pattern ?
		- when our main container needs some prerequisites such as 
		  installing some supportive software, database setup, seeding the DB,
                  permissions on the file system before starting.
		- we can use this pattern to delay the start of the main container.
		
	spec:
	  initContainers:
		- name: fetch
		  image: mwendler/wget
		  command: ["wget","--no-check-certificate","https://sample-videos.com/sql/Sample-SQL-File-1000rows.sql","-O","/docker-entrypoint-initdb.d/dump.sql"]
		  volumeMounts:
			- mountPath: /docker-entrypoint-initdb.d
			  name: dump

2. Sidecar containers: 
	- These are the containers that will run along with the main container.
	- We have a pod with a main container which is working very well but 
	  If we want to extend the functionality without changing the existing 
	  main container then better option is to use sidecar container.
	- To take the copy of file or data from main container we can use 
	  sidecar container.

3. Single Container per Pod
4. Ambassador pattern
5. Adapter pattern
6. Batch Job pattern
7. DaemonSet pattern

=======================================================================================================================================

Kubernetes Networking :
----------------------
-> As a matter of fact, Kubernetes expects US to setup networking to meet certain fundamental requirements.

Pod to pod communication:
-> By default all pods running in a node within a same namespace can communicate with each other without any configuration.
-> A pod in one worker node can access all the pods in the cluster which are in same namespace.	  
-> Containers within same pod can communicate each other. It can be accessed using localhost:port
-> Containers in different pod and node can be accessed using POD IP (Internet Protocol)

How to access containers from outside cluster???????

Kubernetes services:
-------------------
-> Service is an REST API objects with set of rules/policies for accessing set of pods.
-> Services are always created and works at cluster level, not at node level.
-> Services always points to pods directly using labels.
		
-> To list services 
$$ sudo kubectl get svc 
		
Q. Why do we need service?
-> Kubernetes pods are ephemeral in nature. 
-> For eg.:The deployment object can create and destroy pods dynamically. 
-> The set of pods running changes all the time, so even the IP address also. 
-> The service provides static IP address through which the dependent pods/external requests can read the pods.

=======================================================================================================================================-

Q. What are type of loadblancing?
-> In Kubernetes, there are two types of load balancing: internal load balancing and external load balancing.

1. Internal load balancing: 
-> This is used to distribute traffic within the cluster. It allows pods within the cluster to access services by their IP addresses, but it does not expose the service to external traffic. 
-> The ClusterIP service type is an example of internal load balancing.

2. External load balancing: 
-> This is used to distribute traffic from outside the cluster to the pods within the cluster. 
-> It exposes a service to external traffic by mapping it to a load balancer that is provisioned in the underlying infrastructure such as cloud provider's load balancer. 
-> The LoadBalancer service type is an example of external load balancing.

Services brief:
1. Cluster Ip: Cluster ip is to access internally within cluster, Pod to pod communication.
2. Node port: to allow my service/application present in clusture to be accessed by outside world. (30000-32767)
3. Headless service: Here we mention node port as none, so all ips of pods will be listed we can select and run the application, used with Stateful Sets
4. load balancer: Here user traffic will be taken by loadbalancer and it will equally distribute it on target nodes.
5. Ingress controller
6. External Name: DNS  (Domain name server)

*****************************************************************************************************************
1. Cluster IP: Default K8s service (Pod to pod communication)
-> It is the default type of Kubernetes service which exposes the pod IP to the other pods within the cluster.
-> This service is accessed using Kubernetes proxy.
-> Used to solve ephemeral nature of pod to avoid tracking of pod ip we create Cluster Ip to access internally within cluster.
	  
-> Best option include service debugging during development and testing, internal traffic, and dashboards.

apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  selector:
    app: myapp
    type: back-end
  ports:
    - targetPort: 80
      port: 80
     
****************************************************************************************************
2. NodePort:
-> A NodePort service is the most primitive way to get the external traffic directed to our service or application running inside our cluster.
-> The default Load Balancer of Kubernetes is NodePort.
-> Applications running inside the pod will be exposed to the outside world with the use of NodePort and NodeIP, which expose the port on every node.
-> If we wont specify any port while defining NodePort, Kubernetes, it will automatically assigns ports between the range 30000 - 32767
-> Automatically ClusterIP will be created internally.
	     clusterIP + a port mapping to the host port = NodePort
-> NodePort by default opens the specified port in all the worker nodes in the cluster.  
	  
-> Types of ports involved are 
** targetPort – port on the pod (service is forwarded to here)
** port – port on the service itself
** NodePort – port on the node (valid range for node port 30000 – 32767)

apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: Nodeport
  selector:
    app: myapp
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30004

			  
kubectl create -f service-definition.yaml
kubectl get services
kubectl describe svc <servicename>
	  
Yaml file for multiple pods in a node:
-> With label provided, it will filter pods and apply to all pods having the same label.
-> we have multiple similar PODs running our web application. They all have the same labels with a key app set to value myapp. The same label is used as a  selector during the creation of the service. So when the service is created, it looks for matching PODs with the labels and finds 3 of them. The service then automatically selects all the 3 PODs as endpoints to forward the external requests coming from the user. You don’t have to do any additional configuration to make this happen. And if you are wondering what algorithm it uses to balance load, it uses a random  algorithm. Thus the service acts as a built-in load balancer to distribute load across different PODs.

****************************************************************************************************
Headless services:
-> when we neither want load-balancing and a single service IP, use headless service.
-> Headless service lists all the ip's of the pods it is pointing, when a DNS query for headless service is run.	   
-> We can create a headless service by specifying none for the clusterIP.
-> Headless service is used with StatefulSets where name of the pods are fixed.
	
		apiVersion: v1 
		kind: Service 
		metadata:
		  name: headless
		spec:
		  selector: 
		    app: nginx
		  clusterIP: None
		    ports:
		    - name: http 
		      port: 30081
		      targetPort: 80
		      protocol: TCP
			
	 Note: To check the internal working 
			- Login to one of the pod in the group
			- Do nslookup on the services (clusterIP, NodePort and Headless)
			  			  
LoadBalancer:
-> Used to link the external load balancer functionality to the cluster.
-> Typically implemented by a cloud provider and mainly depends on the cloud provider.
-> A network load balancer with an IP address can be used to access the service. 
-> This is not a cost effective way of redirecting the traffic to the cluster.
-> Kubernetes provides a better alternative to this service which is called Ingress Service.

Ingress controller:
-> Ingress Controller is the actual implementation of the Kubernetes Ingress API. 
-> It covers all, layer four to layer seven network services and typically acts as a load balancer by distributing traffic across pods. 
-> It’s also responsible for processing all Ingress resource information.
-> In terms of actual implementation, the Ingress Controller is an application hosted inside a Kubernetes cluster that actively manages a load balancer following Ingress Resources and pre-defined Ingress Rules. It works with all load balancers, including software-based, hardware-based, or cloud service balancers.

-> Selecting the right Kubernetes Ingress Controller depends on our requirements which can be based on the following:
1. 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐒𝐞𝐭: Ensure it supports advanced routing (path-based, host-based, regex), TLS termination, HTTP/2, WebSockets, gRPC, sticky sessions, and URL rewriting.
2.  𝐏𝐞𝐫𝐟𝐨𝐫𝐦𝐚𝐧𝐜𝐞: Choose controllers optimized for high throughput and low latency (e.g., HAProxy or Envoy-based options).
3.  𝐂𝐥𝐨𝐮𝐝 𝐂𝐨𝐦𝐩𝐚𝐭𝐢𝐛𝐢𝐥𝐢𝐭𝐲: Use cloud-native controllers like AWS ALB or GCP Load Balancer for managed environments, or flexible options like NGINX for hybrid/on-prem setups.
4. 𝐄𝐚𝐬𝐞 𝐨𝐟 𝐔𝐬𝐞: Look for simple deployment, automation support, and clear documentation (e.g., Traefik for lightweight clusters).
5. 𝐒𝐞𝐜𝐮𝐫𝐢𝐭𝐲: Check for HTTPS redirection, TLS passthrough, and integration with WAF/DDoS protection.
6.𝐎𝐛𝐬𝐞𝐫𝐯𝐚𝐛𝐢𝐥𝐢𝐭𝐲: Ensure support for Prometheus/Grafana metrics, detailed logging, and traffic analytics.
7.  𝐂𝐨𝐬𝐭 𝐄𝐟𝐟𝐢𝐜𝐢𝐞𝐧𝐜𝐲: Balance resource usage (CPU/memory) with operational expenses for managed or self-hosted options.
-> NGINX is versatile for most use cases, Traefik for simplicity, and cloud-specific controllers for native setups.

-> The Ingress Controller is a crucial component for managing external HTTP and HTTPS traffic into your Kubernetes cluster. 
-> It is responsible for routing requests to the appropriate services based on rules defined in Ingress resources. 
-> The Ingress Controller provides features such as load balancing, SSL termination, and URL-based routing, which are essential for managing traffic in a production environment.

-> There are several popular Ingress Controllers in Kubernetes, such as:
NGINX Ingress Controller
Traefik
HAProxy Ingress
Contour
Azure Application Gateway Ingress Controller

-> Ingress is an API object that provides routing rules to manage external users' access to the services in a Kubernetes cluster, typically via HTTPS/HTTP. 
1. easily set up rules for routing traffic without creating a bunch of Load Balancers or exposing each service on the node. 
2.  best option to use in production environments.
 
->  In production environments, configure and manage content-based routing, support for multiple protocols, and authentication inside the cluster. 
-> An Ingress may be configured to give Services externally-reachable URLs, load balance traffic, 
   terminate SSL / TLS, and offer name-based virtual hosting 
   
-> In Kubernetes, an Ingress backend is a service that the Ingress controller routes traffic to based on the rules defined in an Ingress resource.
-> The backend service is defined as an object in Kubernetes, such as a Service, and is referenced in the Ingress resource by its name. 
   The backend service can be a deployment, a replica set, or any other resource that exposes a network endpoint.
								   Pod
								  /
client---Ingress-managed---->*Ingress*----routing rule--->service
          load balancer			                          \
								   Pod
-> Ingress controllers are typically implemented as a reverse proxy, such as Nginx, Traefik, HAProxy, and Istio.

-> An Ingress provides the following:
1. Externally reachable URLs for applications deployed in Kubernetes clusters
2. Name-based virtual host and URI-based routing support
3. Load balancing rules and traffic, as well as SSL termination
-> Here is a simple example where an Ingress sends all its traffic to one Service:	  

## Expose the NGINX Ingress Controller via LoadBalancer or NodePort ##
-> For production, you would typically expose the NGINX Ingress Controller using a LoadBalancer service (for cloud environments like AWS, GCP, Azure) or a NodePort service if you're managing your own infrastructure.

apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
  labels:
    app: nginx-ingress-controller
spec:
  type: LoadBalancer
  selector:
    app: ingress-nginx
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
    - protocol: TCP
      port: 443
      targetPort: 443


## Create an Ingress Resource ##
-> Once the NGINX Ingress Controller is deployed, you can create an Ingress resource to define routing rules for your services. Here’s an example of an Ingress resource for routing traffic to a backend service:

apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-nginx
  namespace: default
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  rules:
  - host: myapp.example.com  # Replace with your domain
    http:
      paths:
      - path: /app1
        pathType: Prefix
        backend:
          service:
            name: my-app-service  # The name of your backend service
            port:
              number: 80
  tls:
  - hosts:
    - myapp.example.com  # Replace with your domain
    secretName: myapp-tls-secret  # TLS secret for HTTPS termination

========================================================================================================================================================================
ExternalName:
-> Used to define the external DNS (Domain name server) name like cname record or IP address.
-> Maps the Service to the contents of the external Name field (e.g. foo.bar.example.com), by returning a CNAME record with its value. No proxy of any kind is set up.
Eg 
		– if the frontend application is hosted within the cluster 
		and the Database service is hosted externally and in the application
		if we can define the code to access the application like mangoDB 
		and still if want to access it internally then we can define it in External name. 
		
		apiVersion: v1
		kind: Service
		metadata:
			name: MangoDB
		spec:
			type: ExternalName
			externalName: mongoserverDNS name

Istio mesh: (managing microservices)
-> It provides a set of features for managing and securing microservices applications. 
-> It is built on top of the Kubernetes platform and is designed to work seamlessly with it.
-> An Istio service mesh is a configurable infrastructure layer for microservices application that makes communication flexible, reliable, and fast.
## It provides a number of key features, including:
1. Traffic management: controls flow of traffic between microservices, including features such as load balancing, traffic shaping, and fault injection.
2. Service discovery: provides a service discovery mechanism to easily discover and communicate with each other.
3. Security: provides built-in security features, such as mutual Transport Layer Security (TLS) and access control, to help secure communication between microservices.
4. Observability: provides a set of tools for monitoring and troubleshooting microservices, including metrics, logging, and tracing.
5. Configurable: control plane allows to configure and manage the service mesh, and to automate many tasks such as traffic management, security, and observability.

---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Pod Probes: 
-----------
-> Probes are used to obtain the health of an applicaiton running inside a pod's container.
-> Probes can perform periodic call to some applicaiton endpoint within a container which can track the success or failure of application periodically.
-> When subsequent fails occur some user defind triggers can be done with probes.
	
	Advantages of probes 
		- Enable zero downtime deployments.
		- Prevent deployment of broken images.
		- Ensure that failed container are automatically restarted.
		- Can add a delay in starting the application.


** Types of probes:

1. Startup Probe: (initial start of container, gives min startuptime, adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet)

-> This probe will run at the initial start of the container and gives a minimum startup time before running the another probes (liveness, rediness).	
-> Startup probe is the first probe which will be executed among other probes. 
-> This can be used to adopt liveness checks on slow starting containers, avoiding them getting killed by the kubelet before they are up and running.
-> If start up probe succeeds then liveness & readiness will be executed.  
-> It fine tunes the dependencies.
	
	ports:
		- name:liveness-port
		containerPort:8080
		hostPort:8080
		
	livenessProbe:
		httpGet:
			path:/healthz
			port:liveness-port
		failureThreshold:1
		periodSeconds:10

	startupProbe:
		httpGet:
			path:/healthz
			port:liveness-port
		failureThreshold:30
		periodSeconds:10


Liveness Probe: (Health of application, if a container is still running and responsive, if it fails container will be restarted)
----------------
-> A Liviness Probe in Kubernetes checks if a container is still running and responsive. If the liveness probe fails, Kubernetes automatically restarts the container.
-> The livenessProbe is used to determine the health of the applicaiton running inside the pod.
-> If livenessProbe fails the container will be restarted.
-> Ex: Due to some reason like memory leaks in applicaiton or due to high CPU/RAM usage the applicaiton is not responding to our requests, Then in this situation livenessProbe will fail and it will restart the pod.
-> we can define livenessProbe with 3 endpoints 
	
		livenessProbe:
			httpGet:
				path: /test
				port: 8080 
			initialDelaySeconds: 5
			timeoutSeconds: 2 
			periodSeconds: 4

		livenessProbe:
			tcpSocket:
				port: 8080 
			initialDelaySeconds: 5
			timeoutSeconds: 10 
			periodSeconds: 4
			successThreshold: 5
			failuerThreshold: 3
		
		Name port: 	
			ports: 
				- name: line-port	
				  conateinerPort: 8080
				  hostPost: 8080
				  
			livenessProbe:
				httpGet:
					path: /test
					port: line-port

** initialDelaySeconds: After the container started the number of seconds to wait before triggering the probe.	
** timeoutSeconds: Number of seconds after which the probe times out - default/minimun 1 second 
** periodSeconds: How frequently to perform the probe. Default value is 1 second.
** successThreshold: minimum consecutive successes for probe to be considered successful after having failed.
** failureThreshold: no. of times probe tries before giving up/restarting (liveness & startup) or marking as Unready 
		

3. Readiness Probe: (appn is ready to accept traffic, when this prob fails traffic will be halted to this pod and when successful the load balancer traffic is allowed to this pod )
-> A Readiness Probe is used to determine if a application is in a ready state to accept the traffic.
-> When this probe fails traffic will be halted to this pod and when successful the load balancer traffic is allowed to this pod.	  
	  
	- Sometimes, applications are temporarily unable to serve traffic. 
	  For example, an application might need to load large data or 
	  configuration files during startup or depend on external services 
	  after startup. In such cases, you don’t want to kill the application,
	  but you don’t want to send it requests either
	  
	- Readiness and liveness probes can be used in parallel for the same container. 
	  Using both can ensure that traffic does not reach a container that is not 
	  ready for it, and the containers are restarted when they fail.
		
	- Readiness probe can be defines wtith 3 endpoints same as livenessProbe.	
		
Type of probes endpoint: 
1. HTTP/HTTPS endpoint (httpGet)
	- For successful replay we need to get 2XX series replay
	- For failure we need to get 4XX or 5XX http error. 
	- the kubelet sends an HTTP GET request to the server that is running in the container and listening on port 8080.   
	- If the handler for the server's /healthz path returns a success code, the kubelet considers the container to be alive and healthy. 	  
	- If the handler returns a failure code, the kubelet kills the container and restarts it.
			
		HTTP probes fields
			livenessProbe:
				httpGet: 
					path: /mail/u/0/#inbox
					port: 8080
					host: localhost
		host: Host name to connect, the default will be IP of pod.
		path: Path to access on the HTTP server/host.
		port: the port to access on the container.
		
2. TCP endpoints 		
-> In this case kubelet will try to open a tcp socket the port in the container and it will check whether the applicaiton is accessable on that port.
-> For successful replay we need to get 2XX series replay	
		livenessProbe:
			tcpSocket:
			port: 9090
		
3. EXEC commands		
-> Run a shell command, on execution in pod’s shell context and considered failed if the execution returns any result code different from 0 (zero).
			  
		livenessProbe:
			exec:
				command:
					- cat 
					- /etc/temp
			

Type of http status codes 
1. Informational responses ( 100 – 199 ): don't indicate an error. You don't need to troubleshoot these errors.
-> server has received the request and is continuing to process it, about the progress of the request.

2. Successful responses ( 200 – 299 ): no troubleshooting, 
-> the server has successfully processed the request and sent a valid response back to the client.

3. Redirects ( 300 – 399): make sure that the endpoint that the probe is trying to access is 
   correct and that there are no issues with the DNS.

4. Client errors ( 400 – 499 ): indicate that the client has made a mistake in the request.
-> Check the logs of the container to determine the cause of the error.
-> $$ kubectl logs my-pod my-container --since=5m   (logs of my-container in my-pod from last 5min )
-> Make sure that the probe is configured correctly and that the container is listening on the correct port.
-> 404 Not Found: the requested resource was not found on the server. This can occur if 
   the client sends a request for a resource that does not exist or is not accessible.
-> 401 Unauthorized (client needs to authenticate itself ), 403 Forbidden (client does not have permission to access the resource).

5. Server errors ( 500 – 599 ):  indicate that the server was unable to fulfill the request due to an error on the server side.
-> Check the logs of the container to determine the cause of the error. (kubectl logs my-pod my-container --since=5m)
-> Make sure that the container is running and that there are no issues with the container's dependencies.
-> server is not responding, permission error, entire appn is crashed all servers are not workng, permission 
   issue wit firewall, debug these errors, memory exceeded this type of error is coming,delete cookies.
-> 500 Internal Server Error, which indicates that an unexpected error occurred on the server while processing the request. 
   This can occur if there is a bug in the server code, or if there is an issue with the server's infrastructure.
-> 503 Service Unavailable, which indicates that the server is temporarily unable to handle the request due to high traffic 
   or maintenance. This can occur if the server is overloaded or if it is undergoing maintenance.   
-> 502 Bad Gateway, which indicates that there was an issue with a gateway or proxy server.
-> 504 Gateway Timeout, which indicates that a gateway or proxy server did not receive a timely response from an upstream server.

		
------------------------------------------------------------------------------------------------------------------------------------------------------------
 If project has 20-25 worker nodes it is required to create namespace.
------------------------------------------------------------------------------------------------------------------------------------------------------------
Namespaces:
-> Kubernetes namespace is an abstraction to support multiple virtual clusters of k8s objects on the same physical cluster.  
-> Each namespace has its own set of resources, such as pods and services, and can be used to isolate resources within a cluster. 
   Namespaces can also be used to control access to resources, by assigning different roles and permissions to users and groups within each namespace.
	
	Namespaces main functionalities.
		- Namespaces are virtual cluster on top of physical cluster.		
		- Namespaces work at cluster level.
		- within same namespace by default a pod can communicate with other pod.
		- Namespaces provides a logical seperation between environments.
		- Namespaces are only hidden from each other but are not fully isolated, 
		  one service in a NS can talk to another serive in another NS 
		  using fullname like service/object name followed by namespace name 
		
	List namespaces
		sudo kubectl get ns  (or)  sudo kubectl get namespace

Types of default namespaces
	1. default:
		- resources will be created under default if we don’t specify any other namespace
		- if we don’t give namespace then the entire cluster resides in default.
	2. kube-system: 
		- This namespace is for objects created by the kubernetes system.
		- To isolate the Kubernetes master/control plane components (API server, ectd, scheduler, and controller). 	
	3. kube-public: 
		- The objects/resources in this namespace are available or accessable by all. 
		- The objects in this namespace will be public.
		- We never create resources in this namespace until and unless resource should be visible and readable 
                  publicly throughout the cluster.	 
	4. kube-node-lease:
		- This namespace for the lease objects associated with each node which improves the performance 
                  of the node heartbeats as the cluster scales.
       	        - This namespace holds Lease objects associated with each node. Node leases allow the kubelet to send 
		  heartbeats so that the control plane can detect node failure.
		- In Kubernetes, a kube-node-lease is a Lease object that is created by the kubelet on each node. 
		  The kube-node-lease is used to indicate that a node is still "alive" and responsive. The kube-node-lease 
                  is created in the kube-node-lease namespace with the name of the node.

	Create a namespace
		sudo kubectl create namespace <name_of_namespace>
		
	List objects of perticular NS
		sudo kubectl get -n <name_of_namespace> <object_type>
		sudo kubectl get -n test pod
		
	Create object in perticular NS
		matadata: 
			name: ......
			namespace: <name_of_namespace> 
				(or) 	
		sudo kubectl apply -f <filename> -n <name_of_namespace>	
		
	To list the pods in NS
		sudo kubectl get pods -n <ns>
	
	To list the pods in all NS
		sudo kubectl get pods --all-namespaces
		
Namespaces can also be created and attached to pod through config file  

	- To create namespace.yml
	
		apiVersion: v1
		kind: Namespace
		metadata:
			name: test-ns
			
	- To define namespace in deployment config yaml
	
		apiVersion: apps/v1
		kind: Deployment
		metadata:
			name: nginx-deployment
			namespace: test-ns
		spec:
			selector:
				matchLabels:
					app: nginx
				minReadySeconds: 5
			template:
				metadata:
				labels:
					app: nginx
				spec:
					containers:
						- name: nginx
						image: nginx:1.14.2
						ports:
						- containerPort: 80


Enforcing a Security Profile on a Namespace: "POD SECURITY"
========================================================
-> Pod Security in Kubernetes refers to the set of controls and policies that govern the security settings for "Pods" within the cluster. 
-> These controls help ensure that Pods adhere to certain security standards, minimizing the risk of security vulnerabilities, misconfigurations, or malicious activity.
-> PodSecurity mechanism is designed to make enforcing security policies at the Pod level easier and more streamlined, replacing the deprecated PodSecurityPolicy (PSP) feature. The PodSecurity feature is implemented through the "PodSecurity Admission Controller".

** Key Aspects of Kubernetes Pod Security:
-> "PodSecurity Admission Controller" (PSA): The PodSecurity admission controller is used to enforce security policies on Pods at the namespace level. 
-> It is a simpler and more flexible mechanism compared to the older PodSecurityPolicy (PSP) system.

-> Security Profiles: PodSecurity defines three pre-configured security profiles that you can use to enforce policies for your Pods:

PodSecurity Profiles:
====================
-> Each profile defines the security requirements for Pods in a Kubernetes cluster and specifies what actions or configurations are allowed or disallowed.

1. restricted profile:
-> The most stringent security profile. It enforces strict settings that disallow any privileged operations or use of host resources. This is designed to provide a high level of security for your workloads.
-- Disallows running privileged containers.
-- Disallows the use of host network, host PID, and host IPC.
-- Disallows the use of certain unsafe capabilities.
-- Requires running containers with a non-root user.
-- Disallows mounting sensitive files like /etc/passwd or /etc/shadow.

2. baseline profile:
-> A more relaxed profile that allows some flexibility in the security configurations while still enforcing the basics of Pod security.
-- Allows some flexibility, but still limits potentially dangerous operations.
-- Allows non-privileged containers with some limited access to host resources.
-- Allows the use of a host network and other settings for certain workloads if needed.

3. privileged profile:
-> This profile allows Pods to run with fewer restrictions and more flexibility. It is mostly used for workloads that need to run with elevated privileges or access to host resources (e.g., privileged containers).
-- Contains no restrictions. This is the most permissive profile.
-- Allows running containers with any privileges, including privileged containers, host networking, host PID, and access to all host resources.

** How to Enforce Pod Security Policies Using PodSecurity Admission:
---------------------------------------------------------------
-> In Kubernetes, you can configure PodSecurity at the namespace level to enforce security profiles on your Pods. These settings can be applied using annotations in your Namespace resources.

apiVersion: v1
kind: Namespace
metadata:
  name: my-namespace
  labels:
    pod-security.kubernetes.io/enforce: restricted
  pod-security.kubernetes.io/warn
  pod-security.kubernetes.io/audit 

-> pod-security.kubernetes.io/enforce: Specifies the level of enforcement (e.g., restricted, baseline, or privileged).
-> pod-security.kubernetes.io/warn: If set, Kubernetes will issue warnings if Pods in this namespace do not meet the security standards.
-> pod-security.kubernetes.io/audit: Specifies that Pods will be audited but not necessarily blocked or warned.

Enforcement Modes:
-----------------
1. Enforce: Actively enforces the security policies, meaning Pods that do not meet the criteria are rejected.
2. Warn: Issuing a warning for Pods that do not meet security policies, but not blocking them.
3. Audit: Records violations of the policy but does not block or warn users.

How to Enable PodSecurity Admission:
---------------------------------
-> PodSecurity admission is available starting from Kubernetes version 1.22 as an alpha feature and became beta in Kubernetes 1.23.
-> To enable PodSecurity admission, ensure that the feature is enabled on your Kubernetes cluster and configure it according to your needs.
-> You can enable the PodSecurity Admission controller in your Kubernetes cluster by configuring it in the apiServer settings (typically done in the kube-apiserver configuration).

-> Here’s an example for enabling PodSecurity Admission:
$$ --enable-admission-plugins=PodSecurity

Q. Can we have 2 applications in a single namespace?
-> Yes, it is possible to have multiple applications within a single namespace in Kubernetes. A namespace is a logical boundary for resources in a cluster, 
   and it is used to group resources together and provide isolation between them. Each namespace has a unique name and can contain a variety of 
   resources such as pods, services, and deployments.
-> Having multiple applications in a single namespace can be useful for scenarios where the applications are closely related or share similar resources. 
   This can make it easier to manage and organize the resources, and also allows for better resource allocation between the applications. 
   However, it is important to keep in mind that resources in a namespace are not completely isolated from resources in other namespaces, 
   so it's important to have a proper access control and security measures in place.
-> Additionally, having multiple applications in a single namespace can also increase complexity, and make it harder to troubleshoot 
   issues and monitor resources. This could be especially true if the applications are not closely related, have different scaling 
   requirements and have different security requirements. In such cases it is better to have them in different namespaces.

------------------------------------------------------------------------------------------------------------------------------------------------------------
How to create/schedule pods in a particular worker node?
	1. Node selector. 
	2. Afinity and Anti-Afinity. 
        3. Taint and tolerations.

1. Node selector: 
-> Node Selector is a way to bind the pod to a particular worker node, whose label match the nodeSelector lables.
-> Logical expresions type of selection cannot be achived by nodeSelector.		
STEP 1: 		
-> list nodes 
$$ sudo kubectl get nodes 
			
-> Get details of nodes 
$$ sudo kubectl describe nodes 
			
-> Get details of particular node / nodes 
$$ sudo kubectl describe node <node_name>
				
-> list pods with nodes details 
$$ sudo kubectl get pods -o wide					

STEP 2: 	
	- Create a label for the node 
		sudo kubectl label node <node_name> <label_key>=<label_value>
		sudo kubectl label node ip-172-31-46-206 env=test	
STEP 3: 	
	- use nodeSelector field in spec file 
			apiVersion: v1
			kind: Pod
			metadata:
			   name: node-selector
			   labels:
				  env: test
			spec:
			   containers:
				  - name: nginx
					image: nginx
			   nodeSelector:
				  env: test
				  
STEP 4: create the object form STEP 3 and describe the node to check whether the pod is create in the particular node.	


** NOTE:  Hard and Soft rules for affinity & anti-affinity 
	
			- Hard rule - requiredDuringSchedulingIgnoredDuringExecution - With “hard” affinity, 
			  users can set a precise rule that should be met in order for a Pod to be scheduled on a node.
			  
			- Soft rule - preferredDuringSchedulingIgnoredDuringExecution - Using “soft” affinity, you can
			  ask the scheduler to try to run the set of Pod in availability zone XYZ, but if it’s 
			  impossible, allow some of these Pods to run in the other Availability Zone.


2. Node Afinity and Anti-affinity:
a. Node affinity: allows us to schedule the pods to specific nodes with conditional expressions.	
	- Creating pods accross different availability zones to improve the applicaiton availability (resilience).
	- Allocating pods to nodes based on memory-intensive mode. Means create pod based on CPU and RAM availability in worker nodes.

b. Anti-Affinity (Inter-pod affinity)
	- We can define whether a given Pod should or should not be scheduled onto a particular node based on labels.
	
apiVersion: v1
kind: Pod
metadata:
  name: with-pod-affinity
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
      - labelSelector:
          matchExpressions:
          - key: security
            operator: In
            values:
            - S1
        topologyKey: topology.kubernetes.io/zone=V
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
            - key: security
              operator: In
              values:
              - S2
          topologyKey: topology.kubernetes.io/zone=R
  containers:
  - name: with-pod-affinity
    image: registry.k8s.io/pause:2.0

-> This example defines one Pod affinity rule and one Pod anti-affinity rule. 
-> The Pod affinity rule uses the "hard" requiredDuringSchedulingIgnoredDuringExecution, 
-> while the anti-affinity rule uses the "soft" preferredDuringSchedulingIgnoredDuringExecution.

-> The affinity rule says that the scheduler can only schedule a Pod onto a node if the node is in the same zone as one or more existing Pods with the label security=S1. 
-> More precisely, the scheduler must place the Pod on a node that has the topology.kubernetes.io/zone=V label, as long as there is at least one node in that zone that currently has one or more Pods with the Pod label security=S1.

-> The anti-affinity rule says that the scheduler should try to avoid scheduling the Pod onto a node that is in the same zone as one or more Pods with the label security=S2. More precisely, the scheduler should try to avoid placing the Pod on a node that has the topology.kubernetes.io/zone=R label, if there are other nodes in the same zone currently running Pods with the Security=S2 Pod label.

NOTE: 	
	Naming convetion of kubernetes objects name ?
		- contain no more than 253 characters.
		- contain only lowercase alphanumeric characters, '-' or '.'
		- start with an alphanumeric character.
		- end with an alphanumeric character.
		
3. Taint and Tolerations:
-> Taints are used to repel pods from specific nodes.
-> We apply a taint to node which tells the scheduler to repel pods from that worker node.
-> Only pods consisting of toleration for that taint will be created in that worker node.
-> Taint effect defines how nodes with taint react to pods.

-> The master node is already tainted by:
			taints:
			  - effect: NoSchedule
			key: node-role.kubernetes.io/master

	
	STEP 1: 
			- How to taint a node 
				sudo kubectl taint nodes <node_name> <taint_key>=<taint_value>:<taint_effect>
				sudo kubectl taint nodes ip-172-31-46-206 special=true:Schedule
		
		Note: taint key and value can be anything user defined.
		      taint_effect is from kubernetes - NoSchedule, PreferNoSchedule, NoExecute. 
			  1. NoSchedule - Kubernetes will not schedule the pod onto that node.
			  -> The "noschedule" taint is the more restrictive of the two. When a node has a noschedule taint, 
                             no pods can be scheduled on that node, regardless of whether they have tolerations for the taint.
			  2. PreferNoSchedule - Kubernetes will try to not schedule the pod onto the node.
			  -> The preferNoSchedule taint is less restrictive. When a node has a preferNoSchedule taint, 
			     pods will be scheduled on the node if they have a toleration for the taint, but pods without 
                             the toleration will be scheduled on other nodes if possible. This allows for some control over which 
			     pods are scheduled on a node, but does not completely prevent pods from being scheduled on the node.
			  3. NoExecute - the pod will be evicted/deleted from the node (if it is already running on the node), 
				         and will not be scheduled onto the node (if it is not yet running on the node).
			  
	STEP 2: 
			- Adding toleration to pod
			
			apiVersion: v1
			kind: Pod
			metadata:
			   name: node-selector
			   labels:
				  env: test
			spec:
			   containers:
				  - name: nginx
					image: nginx
			   tolerations:
				  - key: "special"
                    value: "true"
                    operator: "Equal"
                    effect: "Schedule"					
					
			apiVersion: v1
			kind: Pod
			metadata:
			   name: node-selector
			   labels:
				  env: test
			spec:
			   containers:
				  - name: nginx
					image: nginx
			   tolerations:
				  - key: "special"
                    operator: "Exists"
                    effect: "Schedule"	
					
		NOTE: The default value for operator is Equal.

			  - A toleration "matches" a taint if the keys are the same and the effects are the same, and:

			  - the operator is Exists (in which case no value should be specified), or
				the operator is Equal and the values are equal.

Q. how to run pod on particular node?
-> $$  kubectl label nodes node-1 environment=production
-> In the pod spec, you will need to include the nodeSelector field and set it to the label you want to match.
-> $$ kubectl create -f pod.yaml   
   (In some cases, you may want to use affinity and anti-affinity rules to ensure that pods are scheduled on 
   specific nodes or avoid other pods.)

------------------------------------------------------------------------------------------------------------------------------------------------------------
Resource quotas and limitaions:
-> When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources.

Q. What is control group/ C-group?
-> It allows for resource management and isolation of processes & to specify limits and constraints on system resources.(CPU, memory, and I/O)
-> Kubernetes, where it is used to ensure that containers are given the resources they need to run, without overloading the host system. 
-> In this context, each container runs in its own cgroup, and K8s can set limits and constraints on the resources used by the container.

The administrator creates one ResourceQuota for each namespace.
-> Resource quotas are the way of restricting teams to use the resources such CPU, RAM and Disk.
-> We can also control the number of objects created in namespace using Object count quota.
-> Resource quotas always works at namespace level.
	
	To check quotas of a namespace 
		sudo kubectl describe ns <namespace_name>
		
	OBJECT COUNT QUOTA 
		List of resources that we can limit via object count quotas
			count/persistentvolumeclaims
			count/services
			count/secrets
			count/configmaps
			count/replicationcontrollers
			count/deployments.apps
			count/replicasets.apps
			count/statefulsets.apps
			count/jobs.batch
			count/cronjobs.batch
					
			apiVersion: v1
			kind: ResourceQuota
			metadata:
				name: object-counts
			spec:
				hard:
				  pods: 2  
				  
	Resource quota on CPU and memory

kubectl describe quota --namespace=myspac
	
apiVersion: v1
kind: Pod
metadata:
   name: node-selector
   labels:
	  env: test
spec:
   containers:
	  - name: nginx
		image: nginx
   resources:
	  requests:
		 memory: "128Mi"	
		 cpu: 0.5
	  limits:
		 memory: "256Mi"
		 cpu: 1
		 
============================================================================================================================
RBAC (Role based access control)
	#Accounts in Kubernetes
	#Roles in kubernetes
	#Binding of roles 

RBAC - Role-Based access control is a method of regulating access to the kubernetes resources based on roles of a account.		
	   
** Key points in RBAC 
Subject: Users, Groups or service accounts 
Resources: Kubernetes objects which need to be operated with RBAC
Verbs: The rules/operations which we want to do with the resources. ("get", "list", "watch", "create", "update", "patch", "delete")
	
	
** There are 2 types of accounts in kubernetes
		
1. USER ACCOUNT: It is used to allow us, humans to access the kubernetes cluster.
			
2. SERVICE ACCOUNT:
------------------- 
-> A Service Account in Kubernetes is an identity that is associated with a set of permissions to interact with the Kubernetes API. 
-> Service accounts are typically used by applications or workloads running inside the cluster to interact with the Kubernetes control plane (such as accessing the Kubernetes API, creating or modifying resources, etc.).

-> It is primarily used to authenticate Pods so that they can access the Kubernetes API and interact with cluster resources securely.
-> It is used to access the API server by other tools and also components inside the clusetr.
-> API server is responsible for such authentication process.
-> If any application running inside a pod or ouside the cluster can access kubernetes cluster using a service account.
-> when a service account is created it first creates a token and keeps that token in a secret object and token can be used by mounting the secret object. 
-> Secret object is linked to the service account.

Service accounts are commonly used for:

-- Allowing Pods to authenticate with Kubernetes APIs.
-- Granting permissions to Pods for specific actions in the cluster.
-- Enabling fine-grained access control through RBAC (Role-Based Access Control).

** Key Features of Service Accounts:
1. Identity for Pods: A service account is assigned to a Pod or Deployment, giving it an identity for API interactions.
2. Role-Based Access Control (RBAC): Service accounts can be associated with RBAC roles to grant permissions to interact with various Kubernetes resources.
3. API Access: Pods using a service account can authenticate to the Kubernetes API server with the associated credentials.

** Components:
-> Service Account: An object representing an identity.
-> Service Account Token: Each service account is given a secret (JWT token) that pods can use to authenticate.
-> RBAC Policies: Service accounts are associated with roles and role bindings to control access permissions.
-> Default Service Account: If no service account is specified, the default service account is used for the pod.

How to Create and Use a Service Account in Kubernetes:
------------------------------------------------------
1. Create a Service Account
Example: Service Account YAML:
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-service-account
  namespace: default  # You can specify a namespace here

$$ kubectl apply -f service-account.yaml    or
$$ kubectl create serviceaccount my-service-account -n default

2. Assigning Roles and Permissions to the Service Account:

a. Create a Role (or ClusterRole) that defines the permissions.
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: default
  name: pod-reader
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]

b. Create a RoleBinding that binds the my-service-account to the pod-reader role.
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: my-service-account-binding
  namespace: default
subjects:
- kind: ServiceAccount
  name: my-service-account
  namespace: default
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io

$$ kubectl apply -f role.yaml
$$ kubectl apply -f rolebinding.yaml

3. Using the Service Account in a Pod
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  serviceAccountName: my-service-account  # Link the service account to the pod
  containers:
  - name: my-container
    image: nginx

$$ apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  serviceAccountName: my-service-account  # Link the service account to the pod
  containers:
  - name: my-container
    image: nginx

4. Accessing the Service Account Token Inside the Pod
-> Kubernetes automatically mounts the service account's token into the pod at the path /var/run/secrets/kubernetes.io/serviceaccount/token. 
-> You can access it from within your application if you need to interact with the Kubernetes API.

-> For example, to access the token from within a container, use:
$$ cat /var/run/secrets/kubernetes.io/serviceaccount/token

Kubectl create sa <sa-name> -n namespace

			apiVersion: v1
			kind: Pod
			metadata:
			   name: monitoring-pod
			spec:
				serviceAccountName: myaccount
				conateiners:
				   .....
			
** Role & ClusterRole
-> Role and ClusterRole contains set of rules to access and modify kubernetes resources. (There are no deny rules)
-> Role is used to set the permissions/rules within a namespace.
-> ClusterRole is cluster wide permissions which is a non-namespaces object.
		
Create a Role 
command line-->kubectl create role my-role --verb=get --verb=get,list --verb=watch --resource=pods

or

apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
   name: my-role
   namespace: test
rules:
   - apiGroups:
        - "*"
     resources:
        - pods
        - services
		- deployments	
     verbs:
		- get
		- list

Create a ClusterRole
  cli-->kubectl create clusterrole my-cluster-role --verb=get,list,watch --resource=pods,deployments
(or)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
   name: my-cluster-role
rules:
   - apiGroups:
        - "*"
     resources:
        - pods
		- deployments	
     verbs:
		- get
		- list
     		- watch

** Role Binding and ClusterRole Binding
-> Role and ClusterRole binding is used to attach the Role to a service account.
-> In Role binding we can bind role to a ClusterRole within a namespace.
	
	RoleBinding 
	
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
   name: my-role
   namespace: test
roleRef: 
   apiGroup: rbac.authorization.k8s.io 
   kind: Role
   name: my-role
subjects:
   - kind: ServiceAccount
     name: myaccount
     namespace: test


-----------------------------------------------------------------------------------------------------------------------------------------
Assignment:
		Create Role and ClusterRole with binding to a service account.
		Resource quotas CPU Memory and count.
-----------------------------------------------------------------------------------------------------------------------------------------
Kubernetes volumes: 
-> Volumes in kubernetes is a kind of directory which is accessable to the containers in the pods.		
 
Types of volumes 
1. emptyDir: It is a volume that is created by default when a pod is first assigned to node, It remains active untill the pod is running.
2. hostPath: Mounts the files or directories from there host nodes.
3. CLoud storages 
	- gcePersistentDisk
	- awsElasticBlockStore (aws EBS)
	- azureDiskVolume
			
1. Persistent Volumes:
-> A PersistentVolume is a piece of storage provisioned in the cluster, either statically or dynamically. It is a cluster-wide resource that exists independently of any specific pod or namespace.
-> It is a piece of storage which can be attached to the pod.
   These are always a cluster level resource not namespace level resource which are maintained by kubernetes.
			
	Create a persistent volume
		apiVersion: v1
		kind: PersistentVolume
		metadata:
		  name: my-pv
		  labels:
		  volume: test	
		spec: 
		  storageClassName: local (or) cloud
		  capacity:
		    storage: 100Gi
		    accessModes:
 		    - ReadWriteOnce
		  hostPath:
		    path: "<path of the host>"(tmp/data)(cloud volume)
		    (path to which we are creating the volumes)
					
2. Persistent Volume Claim
-> This object is used to cliam and mount the required amount of space of volume to pod.
-> Ensure that the PVC has the same storageClassName as the PV.
				apiVersion: v1
				kind: PersistentVolumeClaim
				metadata:
				   name: my-pvc
				spec: 
					storageClassName: local or cloud
					accessModes:
 					   - ReadWriteOnce
					resources:
						resource:
						   storage: 2Gi
					selector:
						matchLanels:
							volume: test				
		  Attach it to pod 	
		  
		    apiVersion: v1
			kind: Pod
			metadata:
			   name: node-selector
			   labels:
				  env: test
			spec:
			   volumes:
				  - name: sample-volume
                    persistentVolumeClaim: 
				 claimName: my-pvc
			   containers:
				  - name: nginx
					image: nginx
			   tolerations:
				  - key: "special"
                    value: "true"
                    operator: "Equal"
                    effect: "NoSchedule"	

Q. Terminology used in PVC claim?
-> It is a request for storage resources that are provisioned by a storage class. used to provide persistent storage for pods and other resources in a cluster.
-> The following are the key terms and concepts used in the context of PVCs:
1. Persistent Volume (PV): A PV is a piece of storage that has been provisioned by an administrator or provisioner. 
   PVs are independent of pods and can be bound to one or more PVCs.
2. Storage Class: A way to provision and manage storage resources for pods and other resources.
3. Claim: A claim is a request for storage resources by a user. A PVC is a specific type of claim that requests storage resources that are backed by a PV.
4. Volume: A volume is a specific instance of a PV that is associated with a PVC. A volume is created when a PVC is bound to a PV.
5. Volume Mode: A volume mode defines whether a volume should be mounted as a file or as a block device.
6. Access Modes: Access Modes are used to specify how a volume can be accessed by a pod. The access modes include ReadWriteOnce, ReadOnlyMany and ReadWriteMany.
7. Volume Binding: The process of associating a PVC with a PV, creating a volume and making it available to a pod.
8. Volume Provisioning: The process of allocating storage resources to a PV.
9. Volume Expansion: The process of increasing the size of a PV after it has been provisioned.
10.Volume Snapshot: A snapshot of a volume's contents at a specific point in time, which can be used to create a new PV.

Q. what is storage class?
-> A way to provision and manage storage resources for pods and other resources. 
  (defines the properties of a specific storage solution such as the performance, durability, and availability of the storage) 
-> A storage class can be used to create a Persistent Volume Claim(PVC) which can be used to request storage resources from the cluster. 
   Once a storage class is created, it can be used by multiple pods to create PVCs that use that class.
-> A storage class can define different parameters such as:
1. The type of storage (e.g. SSD{solid state drive}, HDD{hrd disk drive})
2. The level of replication or availability
3. The performance characteristics (e.g. IOPS, throughput)
4. The size of the storage

Q. What does reclaim policy delete mean?
-> For dynamically provisioned PersistentVolumes, the default reclaim policy is "Delete". This means that a dynamically 
   provisioned volume is automatically deleted when a user deletes the corresponding PersistentVolumeClaim. 
-> This automatic behavior might be inappropriate if the volume contains precious data.

----------------------------------------------------------------------------------------------------------------------------------------------------------------
Q: how do you delete an object file from spec file?
-> To delete an object from a Kubernetes Spec file, you can use the "kubectl edit" command to open the spec file in a text editor, and then manually remove the object definition from the file.
-> how you can delete an object called "my-object" from a spec file called "my-spec.yaml":
   $$ kubectl edit -f my-spec.yaml
-> Alternatively, you can use "kubectl patch" command to update the spec file and remove the object.
   $$ kubectl patch -f my-spec.yaml -p '{"op": "remove", "path": "/spec/template/spec/containers/0"}'

Q. What are helm charts in k8s?  helm:package manager, charts: bundle of yaml playbooks
-> Helm helps you manage Kubernetes applications — Helm Charts help you define, install, manage the lifecycle and upgrade even the most complex 
   Kubernetes application. 
-> Charts are easy to create, version, share, and publish — so start using Helm and stop the copy-and-paste.
-> Helm uses a packaging format called charts, which are collections of Kubernetes manifests that describe the resources needed to run an application.
-> Charts include:
1. A file called Chart.yaml that contains metadata about the chart, such as its name, version, and description.
2. A file called values.yaml that contains default configuration values for the chart.
3. A directory called templates that contains the Kubernetes manifests for the resources defined in the chart.
-> Why do we use Helm charts in Kubernetes?
1. Helm is a package manager for Kubernetes that makes it easy to take applications and services that are either highly repeatable 
   or used in multiple scenarios and deploy them to a typical K8s cluster.
2. Charts can be used to deploy applications, libraries, or any other type of workload on a Kubernetes cluster.

---------------------------------------------------------------------------------------------------------
Q. How to renew certificates in k8s?
-> You can renew your certificates manually at any time with the kubeadm certs renew command. This command performs the renewal using CA (or front-proxy-CA) certificate and key stored in /etc/kubernetes/pki . 
-> After running the command you should restart the control plane Pods.

------------------------------------------------------------------------------------------------------
Q. What is Prometheus and Grapana and how to setup it with k8s?
-> Prometheus is a monitoring and alerting toolkit that collects and stores metrics data from various sources and allows querying and analysis of that data in real-time. 
-> Prometheus will have Prometheus server(Retreival, TSDB, HTTP server) which scrapes and stores time series data.

Setup prometheus on Kubernetes:
-> Most efficient way to deploy proetheus in k8s is using Helm charts. 
-> We can write manifest file and deploy but not preferred. Most efficient helm charts maintained by helm community. 
   No need to create YAML file helm chart already al required info.
-> Helm chharts-Prometheus chart: fork this prject hich as prometheus chart to your account.
-> commands used:
	$$ helm repo add prometheus-community https://prometheus-community.github.i...
	$$ helm install prometheus prometheus-community/prometheus
	$$ kubectl expose service prometheus-server --type=NodePort --target-port=9090 --name=prometheus-server-ext
	$$ minikube service prometheus-server-ext (you can use this interface)
	$$ kubectl get pods/svc/ (rquired prometeus pods will be created)

-> Grafana, on the other hand, is a data visualization and analytics platform that integrates with Prometheus 
   and other data sources to create dashboards, alerts, and other visualizations for monitoring and analysis.

--------------------------------------------------------------------------------------
Q. what is pdb in k8? PodDisruptionBudget
-> PDB (PodDisruptionBudget) to ensure that your pods are not terminated or evicted unless it's absolutely necessary. 
-> A PodDisruptionBudget (PDB) defines the number of replicas of a pod that must be available at any given time for 
   some reason such as upgrades or routine maintenance work on the Kubernetes nodes.
-> ECK manages a default PDB per Elasticsearch resource.

Q. Cordon and drain:
-> Cordon will mark the node as unschedulable.
-> Drain makes it unschedulable and evicts the pods. Safely evict all of your pods from a node before you perform maintenance
   on the node.If any maintenance activity is done on the node then pods cant be evict and moved to another node manually. 
   So drain will be done on the node where it safely evicts the pods to another node, then cordon is used. Make sure there 
   are 2 or 3 nodes available in the cluster with sufficient capacity to maintain the pods.
   
Q. What is kubectl drain? or what is maintainance acheived in K8s?
-> The kubectl drain command is used to safely evict all the pods running on a specific node and move them to other available nodes in the cluster. 
   This command is typically used when a node needs to be taken down for maintenance or replacement.
   $$ kubectl drain <node-name>
   
Q. What is federated clusture in  K8s?
-> A federated Kubernetes cluster is a collection of multiple, geographically dispersed K8s clusters that are managed as a single, unified cluster. This allows you to spread your workloads across multiple clusters and regions, providing high availability and disaster recovery capabilities.
-> Federated clusters are useful for several use cases such as:
1. Multi-cloud deployments: You can deploy your applications across multiple cloud providers for better availability and disaster recovery.
2. Global scale: You can deploy your applications in multiple regions for better performance and availability for your end-users.
3. Hybrid cloud: You can manage your on-premises and cloud-based clusters as a single entity, allowing you to easily move resources between them.

--------------------------------------------------------------------------------------------------
Q. What are operators in k8s?
-> Operators in Kubernetes (k8s) are a way to extend the functionality of the platform by introducing custom resources and custom controllers. 
-> They are used to automate and simplify the management of complex, stateful applications in k8s.
-> Operators uses the Kubernetes API to manage the lifecycle of a specific application or service.
   It does this by creating, configuring, and managing Kubernetes resources on behalf of the user.
-> Operators can be used to automate tasks such as scaling, backup and restore, and upgrades of stateful applications.
*  Two main components:
1. The Operator controller (custom controller): It watches for changes to the custom resource and 
   responds by creating, updating, or deleting the corresponding Kubernetes resources.
2. The custom resource definition (CRD): It defines the desired state of the application or service. 
   Used by operator controller in order to determine what actions to take to bring the application or service to the desired state. 
-> A values file is used in conjunction with an Operator to specify the configuration of an application managed by the Operator.
-> Some common values used in Operators include:
1. Image versions: Specifying the version of the images used to run the application
2. Resource requirements: Defining the resource requirements for the application, such as CPU and memory limits
3. Feature flags: Enabling or disabling specific features of the application
4. Configuration settings: Setting values for various configuration options, such as log level or security settings

Q. How do u secure etcd?: to protect sensitive data and prevent unauthorized access. 
1. Authentication: Enable client authentication to ensure that only authorized clients can access etcd. (client certificates or client authentication tokens)
2. Encryption: Enable transport encryption to ensure that all communication between clients and etcd is secure. (Transport Layer Security (TLS)
3. Authorization: Use etcd's built-in role-based access control (RBAC) to ensure that clients can only access the keys and values.
4. Network segmentation: Use firewall rules and network policies to limit access to etcd to only authorized clients.
5. Backup and restore: Regularly backup etcd data and have a disaster recovery plan in place in case of data loss.
6. Monitor and Audit: Monitor etcd for any suspicious activity and have audit logs to have visibility on etcd's access.
7. Update and patch: Keep etcd up-to-date to ensure that any security vulnerabilities are patched.

Q. Secrets in k8s?
-> A secret is a way to store sensitive information, such as passwords, tokens, and certificates, in a secure and managed way. 
   Secrets are stored as key-value pairs and can be used by pods and other resources in a cluster.
-> The data stored in secrets is encrypted at rest, and only decrypted by the API server when it is sent to a pod.
*  There are two ways to create a secret in Kubernetes:
1. Manually create a secret using the "kubectl create secret" command.
2. Automatically create a secret by a configuration file and use "kubectl apply -f file.yaml" command.
-> Once created, you can use the "kubectl get secrets" command to list all the secrets in a namespace and "kubectl describe secret" to see the details of a secret.
-> Secrets can be consumed by pods in several ways, such as:
*  Mounting secrets as files in a pod's filesystem.
*  Using secrets as environment variables in a pod's containers.
*  Using secrets in configmaps
	$$ kubectl create secret
	$$ kubectl apply -f file.yaml
	$$ kubectl get secrets
	$$ kubectl describe secret
	
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
   
Common #kubernetes pod related errors and ways to mitigate 🤔 ☸
 
➡ ImagePullBackOff
This error appears when #k8s isn't able to "retrieve the image" for one of the #containers of the Pod.
There are three common culprits:
     ✅ The image name is invalid 
     ✅ You specified a non-existing tag for the image.
     ✅ The image that you're trying to retrieve belongs to a private  registry and the cluster doesn't have credentials to access it.
-> The first two cases can be solved by correcting the image name and tag.
-> For the last, one should add the credentials to your private registry in a Secret and reference it in the Pods

➡ RunContainerError
The error appears when the container is unable to start before application
    Common causes:
     ✅ Mounting a not-existent volume such as ConfigMap or Secrets 
     ✅ Mounting a read-only volume as read-write
More detailed aspect can be found by describing the 'failed' pod

➡ CrashLoopBackOff
If the container can't start, then #Kubernetes shows the CrashLoopBackOff message as a status.
Usually, a container can't start when:
  ✅   There's an error in the application that prevents it from starting.
  ✅   You misconfigured the container.
  ✅   The Liveness probe failed too many times.
The "CrashLoopBackOff" error in Kubernetes is usually caused by:
-> Incorrect container image
-> Container image pulled from a private registry that is not accessible to the cluster
-> Insufficient resources (e.g. memory or CPU)
-> Application crashing or exiting with non-zero status code
-> Incorrect environment variables
-> Misconfigured liveness/readiness probes
-> Persistent disk failure
-> Networking issues between containers.

➡ Pods in a Pending state
Assuming that the  scheduler component is running fine, here are the causes:
  ✅   The cluster doesn't have enough resources such as CPU and memory to run the Pod.
  ✅   The current Namespace has a ResourceQuota object and creating the Pod will make the Namespace go over the quota.
  ✅   The Pod is bound to a Pending PersistentVolumeClaim.

The best option is to inspect the Events section in the "kubectl describe"

-----------------------------------------------------------------------------------------------------------------------------------------
Crash loop backoff error:  is a Kubernetes state representing a restart loop that is happening in a Pod: 
-> A container in the Pod is started, but crashes and is then restarted, over and over again. 
-> Kubernetes will wait an increasing back-off time between restarts to give you a chance to fix the error.

*Container is failing beacuse of images is not present.

Types production errors in k8s:
*  Steps to help troubleshoot(commands) and fix the error:
1. Check the container's logs: The first step is to check the logs of the container that is crashing. 
    "kubectl logs <pod-name>". The logs may provide information about the cause of the crash.
-> $$ "kubectl exec" command to run a command in a specific container in a pod.
-> $$ "kubectl get events" command to view recent events related to a pod and the resources in its namespace.
2. Check resource limits: Make sure that the container has enough resources allocated to it. 
    "kubectl describe pod <pod-name>" to check the resource limits and usage.
3. Check the readiness and liveness probes: The readiness and liveness probes can be used to detect and respond to container failures. Check the configuration of these probes and make sure they are configured correctly.
4. Check the environment variables: Make sure that the environment variables passed to the container are correct and that the container has access to the necessary resources.
5. Check the container image: Make sure that the container image is the correct version and that it is not corrupt. 
   Try pulling the image again to ensure it is up to date.
6. Check the application code: If none of the above steps resolve the issue, check the application code 
   for any bugs or issues that may be causing the crash.
7. Last option is to delete the pod and let it recreate.

-> Use Prometheus and Grafana for monitoring and alerting, it helps to get more visibility on resource usage and performance metrics.
-> Use "Kubernetes events" to get more information about the state of the pod.
-> Use the Kubernetes dashboard, which provides a web-based interface for monitoring and managing pods and other resources in the cluster.

Q. What common commands for performing maintenance activities on a Kubernetes node?
-> Upgrading the node's operating system and Kubernetes software:
	$$ sudo apt-get update && sudo apt-get upgrade -y
-> Monitoring the node's resource usage:
	$$ kubectl describe nodes <node-name>
	$$ kubectl top nodes <node-name>
-> Checking for and removing unused or evicted pods:
	$$ kubectl get pods --all-namespaces
	$$ kubectl delete pod <pod-name> --namespace=<namespace>
-> Running a security scan on the node:
	$$ docker run -it --net host --pid host --cap-add audit_control -e DOCKER_CONTENT_TRUST=$DOCKER_CONTENT_TRUST -v 
    /var/lib:/var/lib -v 
    /var/run/docker.sock:/var/run/docker.sock -v 
    /usr/lib/systemd:/usr/lib/systemd -v /etc:/etc --label 
    docker_bench_security docker/docker-bench-security
-> Backing up important data:
	$$ kubectl get pods --all-namespaces -o json > pod-backup.json

https://helm-chart123.s3.us-east-2.amazonaws.com/helmcharts/
arn:aws:s3:::helm-chart123
https://helm-chart123.s3.us-east-2.amazonaws.com/index.yaml
------------------------------------------------------------------------------------------------------------------------------------------------
Q. Deployment Strategies:
-> Deployment strategies define how you want to deliver your software. Organizations follow different deployment strategies based on 
   their business model. Some may choose to deliver software that is fully tested, and others may want their users to provide feedback 
   and let their users evaluate under development features (for example, beta releases). In the following section we will talk about 
   various deployment strategies.
1. In-Place Deployments
2. Blue/Green Deployments
3. Canary Deployments
4. Linear Deployments
5. All-at-once Deployments

1. In-Place Deployments:
-> In this strategy, the deployment is done when the application on each instance in the deployment group is stopped, the latest application revision is installed, and the new version of the application is started and validated. 
-> You can use a load balancer so that each instance is deregistered during its deployment and then restored to service after the deployment is complete. 
-> In-place deployments can be all-at-once, assuming a service outage, or done as a rolling update. AWS CodeDeploy and AWS Elastic Beanstalk offer 
   deployment configurations for one at a time, half at a time and all at once. These same deployment strategies for in-place 
   deployments are available within blue/green deployments.

2. Blue/Green Deployments  or Red/black deployment:
-> It is a technique for releasing applications by shifting traffic between two identical environments
   running differing versions of the application. 
-> Blue/green deployments help you minimize downtime during application updates mitigating risks surrounding downtime 
   and rollback functionality. 
-> Blue/green deployments enable you to launch a new version (green) of your application alongside the old version (blue), 
   and monitor and test the new version before you reroute traffic to it, rolling back on issue detection.

3. Canary Deployments:
-> Traffic is shifted in two increments. A canary deployment is a blue/green strategy that is more risk-averse, in which a phased approach is used. 
-> This can be two step or linear in which new application code is deployed and exposed for trial, and upon acceptance 
   rolled out either to the rest of the environment or in a linear fashion.

4. Linear Deployments: 
-> Linear deployments mean that traffic is shifted in equal increments with an equal number of minutes between each increment. 
   You can choose from predefined linear options that specify the percentage of traffic shifted in each increment and the number 
   of minutes between each increment.

5. All-at-once Deployments
-> All-at-once deployments mean that all traffic is shifted from the original environment to the replacement environment all at once.

---------------------------------------------------------------------------------------------------------------------------------------------------------
Q. what is meant by high availability of clusture?
-> High availability (HA) in a cluster refers to the ability of the cluster to continue functioning properly even 
   in the event of a failure or outage. This means that if one node or component of the cluster goes down, 
   the other nodes can take over and continue providing the service without interruption. 
-> A highly available cluster typically includes multiple nodes, with each node running the same service and a 
   mechanism for automatically detecting and responding to failures. This can be achieved by using a load balancer, 
   and various type of replication.

-------------------------------------------------------------------------------------------------------------------------------------
Q. Types of errors and how will u troubleshoot them in k8s?
-> There are several types of errors that can occur in a Kubernetes cluster, and the troubleshooting process will depend on the specific error. 
  Here are a few common types of errors and some general troubleshooting steps for each:

1. Pod and container errors: These include issues such as "CrashLoopBackOff" and "ErrImagePull". 
   To troubleshoot these errors, check the container logs using "kubectl logs", check the resource limits using "kubectl describe pod", 
   and check the readiness and liveness probes.
2. Networking errors: These include issues such as "Connection refused" and "Error forwarding ports". To troubleshoot these errors, 
   check the network configuration, check for firewall rules blocking the traffic, check the pod-to-pod and service-to-service communication.
3. Scheduling errors: These include issues such as "No resources available" and "NodeNotReady". To troubleshoot these errors, 
   check the resource usage on the nodes, check the resource limits and requests for pods, check the node conditions.
4. Deployment errors: These include issues such as "Failed to create deployment" and "Invalid configuration". To troubleshoot these errors, 
   check the deployment configuration, check the environment variables, check the image version, check the application code.
5. Persistent Volume Claim errors: These include issues such as "PVC not found" or "Unable to bind PVC". To troubleshoot these errors, 
   check the PVC and PV status, check the storage class, check the access modes, check the storage capacity.
6. Authentication and Authorization errors: These include issues such as "Forbidden" and "Unauthorized". To troubleshoot these errors, 
   check the service account, check the role-based access control, check the authentication and authorization method.

---------------------------------------------------------------------------------------------------------------------------------------------------------
Q. How to access the application using kunernetes?
-> To access an application deployed in a Kubernetes cluster, you need to use a Service resource. A Service defines the network 
   access to a set of replicas of an application. There are several types of Services in Kubernetes, including ClusterIP, 
   NodePort, LoadBalancer, and ExternalName, each with different features and use cases.
-> Here are the general steps to access an application using Kubernetes:
1. Create a Deployment for the application: This defines the desired state for the replicas of the application.
2. Create a Service for the Deployment: This defines the network access to the replicas, such as the IP address, port, and target selector.
3. Access the application using the Service's IP address and port: You can access the application by connecting 
   to the Service's IP address and port from within the cluster or from outside the cluster. 
   The exact method depends on the type of Service you created.
-> For example, if you created a ClusterIP Service, you can access the application by connecting to the Service's IP address 
   and port within the cluster. If you created a LoadBalancer Service, you can access the application by connecting to the 
   external IP address assigned by the cloud provider.
   
Q. What is secret ashmaps in k8s ?
-> A Secret is an object that contains a small amount of sensitive data such as a password, a token, or a key. 

Q. How to debug the nodes in k8s ?
	1.Reconfiguring a kubeadm cluster.
	2.Changing the Container Runtime on a Node from Docker Engine to containerd. ...
	3.Generate Certificates Manually.
	4.Reconfigure a Node's Kubelet in a Live Cluster.
	5.Using NodeLocal DNSCache in Kubernetes Clusters.
	6.Verify Signed Kubernetes Artifacts.
	
Q. How to troubleshoot the k8s cluster ?
	1.Administer a Cluster. Reconfiguring a kubeadm cluster. ...
	2.Use a User Namespace With a Pod.
	3.Monitoring, Logging, and Debugging. Troubleshooting Applications. ...
	4.Horizontal Pod Autoscaling. ...
	5.Job with Pod-to-Pod Communication. ...
	6.Deploy and Access the Kubernetes Dashboard.
	7.Use a SOCKS5 Proxy to Access the Kubernetes API.

Q. How we can do the storages in k8s ?
-> A Kubernetes cluster stores all its data in etcd.
	
Q. How Kubernetics manifest looks like?  Explain Kubernetics manifest yml file.
-> Kubernetes manifests ( .yaml or .json) are used to create, modify and delete Kubernetes resources such as pods, deployments, services or ingresses
   pp requirement is push to azure container and  deploy through Aks,

Q. you are able to deploy app to K8 service A developer is saying the app is down. How will You trouble shoot app other than using probes How to trouble shoot the AKS cluster?
kubectl get pods [check status of pods]
kubectl get logs application[wil be chceked for dependencies]
kubectl describe pod <pod name>[to view appl related to pods]
kubectl describe service <service name>[to view service related to pods]
the pod will be deleted and restarted

Q. Explain Kubectl cmds?
kubectl run <pod-name> --image=<image-name>
kubectl create cm <configmap-name> --from-literal=<key>=<value>
kubeclt create cm <configmap-name> --from-file=<file-name>
kubectl create deployment <deployment-name> --image=<image-name>
kubectl scale deployment <deployment-name> --replicas=<new-replica-count>
kubectl delete pod <pod name>

Q. Some person is hitting the k8 cluster from browser how to  make the app visible to him.
one need to have access to the cluster then he can use kubectl commands to access the cluster
kubectl -n kube-system edit service kubernetes-dashboard
kubectl -n kube-system get services

Q. In K8 manifest file we have to use  secrets in k8 namespace how to  inject to k8secrets into ns
$ kubectl create namespace testns1
[namespace/testns1 created]
$ kubectl create secret generic test-secret-1 --from-literal=username=test-user --from-literal=password=testP@ssword -n testns1
[secret/test-secret-1 created]

apiVersion: v1
kind: Namespace
metadata:
  name: development
  labels:
    name: development
	
---------------------------------------------------------------------------------------------------------------------------------------------------
HTTP AND HTTPS:
-> HTTP (Hypertext Transfer Protocol) and HTTPS (Hypertext Transfer Protocol Secure) are both protocols used for transmitting data over the internet.
-> HTTP is the foundation of data communication on the web and is used to request and transmit data 
   between servers and clients. HTTP is a stateless protocol, which means that each request and response 
   is independent of previous requests and responses.
-> HTTPS is a more secure version of HTTP. It uses a combination of HTTP and SSL/TLS (Secure Sockets Layer/Transport Layer Security) 
   protocols to encrypt data transmitted between a client and a server, which helps to prevent unauthorized access, 
   data interception, and other security threats.
-> HTTPS is identified by the "https://" prefix in a website's URL, and it uses port 443 instead of port 80, which is used by HTTP. 
   HTTPS is commonly used for transmitting sensitive information such as passwords, credit card numbers, and other personal data.
-> In summary, HTTP is the basic protocol for transmitting data on the web, while HTTPS is a more secure version of HTTP that 
   uses encryption to protect data. When transmitting sensitive information, it's important to use HTTPS to ensure that 
   the data is protected from unauthorized access and interception.

** SSL (Secure Sockets Layer) and TLS (Transport Layer Security) are cryptographic protocols that are used to 
   secure communication over the internet. SSL was the original protocol developed by Netscape in the mid-1990s, while TLS is its successor.
-> SSL/TLS is used to encrypt data transmitted between a client and a server, providing confidentiality, integrity, 
   and authentication, which helps to prevent unauthorized access, data interception, and other security threats. 
   It is commonly used for securing sensitive data such as passwords, credit card numbers, and other personal information.
-> When a client and a server establish an SSL/TLS connection, they go through a process called the handshake, which includes the following steps:
1. The client sends a hello message to the server, which includes the client's SSL/TLS version, 
   a list of supported cipher suites, and other information.
2. The server responds with a hello message, which includes the server's SSL/TLS version, the chosen cipher suite, and other information.
3. The server sends its SSL/TLS certificate to the client, which is used to authenticate the server.
4. The client verifies the server's certificate and, if the certificate is valid, generates a random key that 
   will be used to encrypt the data transmitted between the client and the server.
5. The client encrypts the random key using the server's public key, which is included in the server's 
   certificate, and sends the encrypted key to the server.
6. The server decrypts the random key using its private key and uses the key to encrypt data transmitted between the client and the server.

-> Once the SSL/TLS connection is established, all data transmitted between the client and the server is encrypted 
   and can only be decrypted by the intended recipient.
-> In summary, SSL/TLS is a cryptographic protocol used to secure communication over the internet. 
   It provides confidentiality, integrity, and authentication, which helps to prevent unauthorized access, 
   data interception, and other security threats.

Q. if u delete a pod what else will delete?
-> When a Kubernetes pod is deleted, the following resources associated with the pod will also be deleted:
1. Containers: The containers running inside the pod will be terminated and deleted.
2. Volumes: Any volumes attached to the pod will be unmounted and deleted.
3. Services: If the pod is part of a service, the service endpoints will be updated to remove the pod.
4. Replication controllers or deployments: If the pod is part of a replication controller or deployment, 
   the controller will create a new pod to maintain the desired number of replicas.
5. Stateful sets: If the pod is part of a stateful set, the stateful set controller will create a new pod 
   with the same name to maintain the desired state.
6. Daemon sets: If the pod is part of a daemon set, the daemon set controller will create a new pod on the 
   same node to maintain the desired state.
7. Jobs or CronJobs: If the pod is part of a job or CronJob, the job controller will create a new pod to complete the job.
-> It's important to note that deleting a pod in Kubernetes is not the same as stopping or killing a process on 
   a traditional server. Kubernetes will attempt to replace the pod to maintain the desired state of the application.

Q. how u manage kubernetes using jenkins pipeline?
-> There are several ways to manage Kubernetes using Jenkins pipeline. Here are the high-level steps to manage Kubernetes using Jenkins pipeline:
1. Install the Kubernetes plugin: To manage Kubernetes using Jenkins pipeline, you need to install 
   the Kubernetes plugin. This plugin provides integration between Jenkins and Kubernetes, allowing 
   you to create, deploy, and manage Kubernetes resources directly from your Jenkins pipeline.
2. Create a Kubernetes configuration file: To interact with your Kubernetes cluster, you need to create 
   a configuration file that contains the necessary details of your Kubernetes cluster, such as the API 
   server URL, certificate authority, and token or username/password for authentication.
3. Define Kubernetes deployment in Jenkinsfile: In your Jenkinsfile, define the Kubernetes deployment, 
   service, or other resources that you want to create or update. This can be done using the Kubernetes 
   plugin's DSL, which provides a set of functions to interact with the Kubernetes API.
4. Use Kubernetes plugin steps in pipeline: Use Kubernetes plugin's pipeline steps such as kubernetesDeploy, 
   kubernetesRollback, kubernetesScale, kubernetesVerifyDeploy, and others to manage Kubernetes resources such 
   as deployments, services, ConfigMaps, and Secrets.
5. Build and deploy: Build and deploy your application to Kubernetes using your Jenkins pipeline. 
   Your pipeline should include the steps to build your application code, containerize the application, 
   push the container image to a container registry, and deploy the application to Kubernetes using the Kubernetes plugin.
-> By using Jenkins pipeline to manage Kubernetes, you can automate the deployment and management of your 
   applications in Kubernetes, ensuring that your applications are always up-to-date and running as expected. 
   This approach also allows you to easily manage multiple Kubernetes clusters and deploy applications to different 
   environments, such as development, testing, and production.
