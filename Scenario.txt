#Scenario1. 
Q. While running a .#Terraform pipeline, the .#Devops engineer canceled the process during the terraform apply phase. This left the Terraform state locked in S3 and DynamoDB. Later, after making changes to the Terraform code, the engineer attempted to rerun the pipeline, but encountered an error stating that the state was locked. This blocked the pipeline from proceeding further. 
-> 
#Solution:
Here's how he resolved it
1ï¸âƒ£ Configured the remote backend locally using the same backend configuration as the pipeline. This ensured the local setup matched the pipeline's state storage settings
ğğ±ğšğ¦ğ©ğ¥ğ ğ›ğšğœğ¤ğğ§ğ ğœğ¨ğ§ğŸğ¢ğ  
ğ˜µğ˜¦ğ˜³ğ˜³ğ˜¢ğ˜§ğ˜°ğ˜³ğ˜® {
 ğ˜£ğ˜¢ğ˜¤ğ˜¬ğ˜¦ğ˜¯ğ˜¥ "ğ˜´3" {
 ğ˜£ğ˜¶ğ˜¤ğ˜¬ğ˜¦ğ˜µ     = "ğ˜®ğ˜º-ğ˜µğ˜¦ğ˜³ğ˜³ğ˜¢ğ˜§ğ˜°ğ˜³ğ˜®-ğ˜´ğ˜µğ˜¢ğ˜µğ˜¦-ğ˜£ğ˜¶ğ˜¤ğ˜¬ğ˜¦ğ˜µ"
 ğ˜¬ğ˜¦ğ˜º      = "ğ˜¦ğ˜¯ğ˜·/ğ˜¥ğ˜¦ğ˜·/ğ˜µğ˜¦ğ˜³ğ˜³ğ˜¢ğ˜§ğ˜°ğ˜³ğ˜®.ğ˜µğ˜§ğ˜´ğ˜µğ˜¢ğ˜µğ˜¦"
 ğ˜³ğ˜¦ğ˜¨ğ˜ªğ˜°ğ˜¯     = "ğ˜¦ğ˜¶-ğ˜¸ğ˜¦ğ˜´ğ˜µ-3"
 ğ˜¥ğ˜ºğ˜¯ğ˜¢ğ˜®ğ˜°ğ˜¥ğ˜£_ğ˜µğ˜¢ğ˜£ğ˜­ğ˜¦ = "ğ˜®ğ˜º-ğ˜µğ˜¦ğ˜³ğ˜³ğ˜¢ğ˜§ğ˜°ğ˜³ğ˜®-ğ˜­ğ˜°ğ˜¤ğ˜¬-ğ˜µğ˜¢ğ˜£ğ˜­ğ˜¦"
 }
}

2ï¸âƒ£ Ran ğ­ğğ«ğ«ğšğŸğ¨ğ«ğ¦ ğ¢ğ§ğ¢ğ­ to Initialize Terraform to sync the backend configuration
3ï¸âƒ£ Ran ğ­ğğ«ğ«ğšğŸğ¨ğ«ğ¦ ğŸğ¨ğ«ğœğ-ğ®ğ§ğ¥ğ¨ğœğ¤ <ğ‹ğğ‚ğŠ_ğˆğƒ> to forcefully remove the lock, which basically removes the lock from backend.
4ï¸âƒ£ Resumed the pipeline again.

While using terraform force-unlock can resolve locked state issues, it is not advisable to force-unlock the state unless you are certain the lock was caused by your actions (e.g., canceling a pipeline or Terraform process).
Forcing an unlock while another process is genuinely using the state can lead to state corruption or inconsistent infrastructure changes. Always double-check that no active Terraform processes are running.

=====================================================================
#Scenario 2
** A large organization manages multiple AWS accounts and VPCs across several regions, each supporting a different business unit. The companyâ€™s .#Cloud & .#DevOps teams frequently provision new VPCs and subnets for application deployments, but they are running into overlapping IP addresses, causing routing conflicts and connectivity issues. The teams also lack visibility into the IP address usage across different accounts, leading to inefficient IP allocation, potential security risks, and costly reconfiguration.

#Challenge
Q. How can the organization manage IP address allocation across multiple AWS accounts and regions to avoid IP conflicts, ensure efficient use of address space, and maintain compliance with IP management ?

#Solution
AWS IP Address Manager (.#IPAM) can streamline the organizationâ€™s IP management by providing centralized control, monitoring, and visibility. Hereâ€™s how IPAM addresses this challenge:

1. ğ‚ğğ§ğ­ğ«ğšğ¥ğ¢ğ³ğğ ğˆğ ğ€ğğğ«ğğ¬ğ¬ ğŒğšğ§ğšğ ğğ¦ğğ§ğ­ IPAM allows the organization to define IP pools and allocate CIDR blocks to each business unit's VPCs in multiple accounts and regions. This eliminates the need for manual tracking and reduces the risk of overlapping IPs.

2. ğ€ğ®ğ­ğ¨ğ¦ğšğ­ğğ ğŒğ¨ğ§ğ¢ğ­ğ¨ğ«ğ¢ğ§ğ  ğšğ§ğ ğ‚ğ¨ğ¦ğ©ğ¥ğ¢ğšğ§ğœğ IPAM automatically monitors IP usage, ensuring that each allocated CIDR follows the organizationâ€™s compliance rules. Alerts are generated when IP usage exceeds defined limits or if a conflict arises.

3. ğ„ğŸğŸğ¢ğœğ¢ğğ§ğ­ ğˆğ ğ€ğ¥ğ¥ğ¨ğœğšğ­ğ¢ğ¨ğ§ ğšğ§ğ ğ‘ğğ©ğ¨ğ«ğ­ğ¢ğ§ğ  IPAM provides insights into IP utilization across accounts, enabling teams to make informed decisions and better plan future allocations, thus avoiding underused or overused address spaces.

Using AWS IPAM improves visibility and control over IP address allocation, preventing conflicts, enhancing security, and simplifying network management across a complex, multi-account AWS environment.

================================================================
#Terraform is a powerful .#IAC tool for provisioning infrastructure, but thereâ€™s a lot going on under the hood every time you hit ğšğ©ğ©ğ¥ğ²

ğŸ“œ Step 1: Parses and validates the .tf configurations, checking for syntax, variable correctness, and dependencies.
ğŸ”„ Step 2: Refreshes the state to ensure Terraform has the latest details on all resources, from the local file or a remote backend.
ğŸ“Š Step 3: Creates a detailed execution planâ€”outlining what will be created, updated, or destroyed.
ğŸ”’ Step 4: Sends secure API requests to AWS, using HTTPS and IAM authentication to create or modify each resource.
ğŸ“ Step 5: Updates the state file to reflect the new infrastructure, whether stored locally or remotely.

====================================================================================
In the world of .#Dev & .#DevOps, .ğ¢ğ ğ§ğ¨ğ«ğ files are often overlooked but are essential for optimizing workflows, ensuring security, and keeping things clean. Here's a quick summary of some crucial `.ignore` files and why they matter.

 1ï¸âƒ£ .ğ ğ¢ğ­ğ¢ğ ğ§ğ¨ğ«ğ
- ğ‘·ğ’–ğ’“ğ’‘ğ’ğ’”ğ’†: Prevent unnecessary files (like `node_modules` or environment files) from being committed to your Git repository.
- ğ‘°ğ’ğ’‘ğ’‚ğ’„ğ’•: Keeps your repository clean and protects sensitive data.

 2ï¸âƒ£ .ğğ¨ğœğ¤ğğ«ğ¢ğ ğ§ğ¨ğ«ğ
- ğ‘·ğ’–ğ’“ğ’‘ğ’ğ’”ğ’†: Exclude files from your Docker image build context to reduce image size and speed up builds.
- ğ‘°ğ’ğ’‘ğ’‚ğ’„ğ’•: Makes your Docker images lightweight and efficient.

 3ï¸âƒ£  .ğ¡ğğ¥ğ¦ğ¢ğ ğ§ğ¨ğ«ğ
- ğ‘·ğ’–ğ’“ğ’‘ğ’ğ’”ğ’†: Ignore files when packaging Helm charts to deploy applications on Kubernetes.
- ğ‘°ğ’ğ’‘ğ’‚ğ’„ğ’•: Keeps your Helm charts compact and deployment-ready.

 4ï¸âƒ£ .ğœğŸğ¢ğ ğ§ğ¨ğ«ğ
- ğ‘·ğ’–ğ’“ğ’‘ğ’ğ’”ğ’†: Specify which files should be excluded when pushing an app to Cloud Foundry.
- ğ‘°ğ’ğ’‘ğ’‚ğ’„ğ’•: Ensures that only necessary code and resources are deployed.

 5ï¸âƒ£ .ğ­ğğ«ğ«ğšğŸğ¨ğ«ğ¦ğ¢ğ ğ§ğ¨ğ«ğ
- ğ‘·ğ’–ğ’“ğ’‘ğ’ğ’”ğ’†: Ignore files in a Terraform module when initializing or publishing to the Terraform Registry.
- ğ‘°ğ’ğ’‘ğ’‚ğ’„ğ’•: Helps maintain a focused and efficient module structure.

 6ï¸âƒ£ .ğ©ğ«ğğ­ğ­ğ¢ğğ«ğ¢ğ ğ§ğ¨ğ«ğ & .ğğ¬ğ¥ğ¢ğ§ğ­ğ¢ğ ğ§ğ¨ğ«ğ
- ğ‘·ğ’–ğ’“ğ’‘ğ’ğ’”ğ’†: Exclude files from code formatting and linting tools (Prettier and ESLint).
- ğ‘°ğ’ğ’‘ğ’‚ğ’„ğ’•: Saves time by avoiding checks on files that donâ€™t need formatting or linting.

ğŸ‘‰ Pro Tip: Review your `.ignore` files regularly to ensure they are up to date and aligned with your current project requirements.

========================================================================================
#Scenario: A DevOps Engineer is working on multiple projects simultaneously, each requiring commits with different Git user profiles. The engineer contributes to client projects, open-source projects and internal company repositories, using different email addresses and usernames for each. 

.#Challenge: The DevOps Engineer struggles with efficiently managing multiple Git user profiles for different repositories. Each time the engineer switch projects, he have to update the Git configuration which is time consuming and may lead to errors which may lead to misunderstandings with collaborators and potential issues with maintaining proper code ownership.

.#Solution: To make the process of switching Git profiles easier, the DevOps Engineer creates convenient shell aliases for configuring Git user settings. ğ‡ğğ«ğâ€™ğ¬ ğ¡ğ¨ğ° ğ­ğ¨ ğğ¨ ğ¢ğ­:
1. Open the shell configuration file, such as ~/.bashrc or ~/.zshrc, using a text editor
ğ’—ğ’Š ~/.ğ’ƒğ’‚ğ’”ğ’‰ğ’“ğ’„ ğ’ğ’“ ~/.ğ’›ğ’”ğ’‰ğ’“ğ’„

2.Then add the following aliases at the end of the file, below are example users and emails.
ğ’‚ğ’ğ’Šğ’‚ğ’” ğ’ˆğ’Šğ’•-ğ’‚ğ’ğ’‚ğ’“="ğ’ˆğ’Šğ’• ğ’„ğ’ğ’ğ’‡ğ’Šğ’ˆ ğ’–ğ’”ğ’†ğ’“.ğ’ğ’‚ğ’ğ’† â€˜ğ’‚ğ’ğ’‚ğ’“' && ğ’ˆğ’Šğ’• ğ’„ğ’ğ’ğ’‡ğ’Šğ’ˆ ğ’–ğ’”ğ’†ğ’“.ğ’†ğ’ğ’‚ğ’Šğ’ '****@ğ’ˆğ’ğ’‚ğ’Šğ’.ğ’„ğ’ğ’'"
ğ’‚ğ’ğ’Šğ’‚ğ’” ğ’ˆğ’Šğ’•-ğ’‚ğ’Œğ’ƒğ’‚ğ’“="ğ’ˆğ’Šğ’• ğ’„ğ’ğ’ğ’‡ğ’Šğ’ˆ ğ’–ğ’”ğ’†ğ’“.ğ’ğ’‚ğ’ğ’† â€˜ğ’‚ğ’Œğ’ƒğ’‚ğ’“' && ğ’ˆğ’Šğ’• ğ’„ğ’ğ’ğ’‡ğ’Šğ’ˆ ğ’–ğ’”ğ’†ğ’“.ğ’†ğ’ğ’‚ğ’Šğ’ '****@***.ğ’„ğ’ğ’'"
ğ’‚ğ’ğ’Šğ’‚ğ’” ğ’ˆğ’Šğ’•-ğ’‚ğ’ğ’•ğ’‰ğ’ğ’ğ’š="ğ’ˆğ’Šğ’• ğ’„ğ’ğ’ğ’‡ğ’Šğ’ˆ ğ’–ğ’”ğ’†ğ’“.ğ’ğ’‚ğ’ğ’† 'ğ’‚ğ’ğ’•ğ’‰ğ’ğ’ğ’š' && ğ’ˆğ’Šğ’• ğ’„ğ’ğ’ğ’‡ğ’Šğ’ˆ ğ’–ğ’”ğ’†ğ’“.ğ’†ğ’ğ’‚ğ’Šğ’ '****@****.ğ’„ğ’ğ’'"

3.Then save the file and reload the shell to apply the changes
ğ’”ğ’ğ’–ğ’“ğ’„ğ’† ~/.ğ’ƒğ’‚ğ’”ğ’‰ğ’“ğ’„ or ğ¬ğ¨ğ®ğ«ğœğ ~/.ğ³ğ¬ğ¡ğ«ğœ or the file you changed

4. Now, you can quickly switch between Git user profiles within any GitHub repository by simply typing
ğ’ˆğ’Šğ’•-ğ’‚ğ’ğ’‚ğ’“ and press enter to use the "ğšğ¦ğšğ«" profile.
ğ’ˆğ’Šğ’•-ğ’‚ğ’Œğ’ƒğ’‚ğ’“ press enter to switch to the "ğšğ¤ğ›ğšğ«" profile.
ğ’ˆğ’Šğ’•-ğ’‚ğ’ğ’•ğ’‰ğ’ğ’ğ’š press enter to activate the "ğšğ§ğ­ğ¡ğ¨ğ§ğ²" profile.


================================================================
As a Cloud and Devops engineer, imagine you are tasked to run your microservices on AWS ECS. The microservice architecture needs service to service communication for the whole system to work like serviceA needs to communicate with ServiceB and there are more services likewise. So, Letâ€™s look at some of the methods along with their complications so that it will easier for you to choose the best one based on your requirements.

1.ğ„ğ¥ğšğ¬ğ­ğ¢ğœ ğ‹ğ¨ğšğ ğğšğ¥ğšğ§ğœğ¢ğ§ğ 
ELB facilitates communication by distributing incoming traffic evenly across ECS tasks, ensuring that all services receive requests efficiently. It provides health checks and automatically routes traffic to healthy instances.
ğ‚ğ¨ğ¦ğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ğ¬: ELB needs to be carefully planned for configuring infrastructure for high availability and incur additional infrastructure cost.

2.ğ€ğ¦ğšğ³ğ¨ğ§ ğ„ğ‚ğ’ ğ’ğğ«ğ¯ğ¢ğœğ ğƒğ¢ğ¬ğœğ¨ğ¯ğğ«ğ²
Service Discovery simplifies inter-service communication by dynamically registering and discovering services using AWS Cloud Map or DNS. Microservices can locate each other through service names rather than static IPs, enabling flexibility and scalability as the infrastructure evolves.
ğ‚ğ¨ğ¦ğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ğ¬ : It often requires developers to write custom application code for collecting traffic metrics and for making network calls resilient

3. ğ’ğğ«ğ¯ğ¢ğœğ ğ¦ğğ¬ğ¡ğğ¬ ğ¥ğ¢ğ¤ğ ğ€ğ–ğ’ ğ€ğ©ğ© ğŒğğ¬ğ¡
It provides a powerful layer for managing service-to-service communication. They enable fine-grained traffic control, advanced routing capabilities.
ğ‚ğ¨ğ¦ğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ğ¬: It run outside of Amazon ECS despite having advanced traffic monitoring and routing features between services.

4. ğ„ğ‚ğ’ ğ’ğğ«ğ¯ğ¢ğœğ ğ‚ğ¨ğ§ğ§ğğœğ­ 
It offers a streamlined way to manage communication between services natively within ECS. It automates service registration and connection, making inter-service communication straightforward. This method also provides built-in load balancing and simplifies configurations, ensuring that services connect and communicate efficiently without extensive manual setup.
ğ‚ğ¨ğ¦ğ©ğ¥ğ¢ğœğšğ­ğ¢ğ¨ğ§ğ¬: Limited advanced features compared to service meshes. It is relatively a new method compared to others.

================================================================
.#Scenario
You're a DevOps engineer managing a microservices application on a Production Kubernetes cluster. One day, your `auth-service`, which handles user authentication, starts crashing and experiencing delays. You try looking at logs and metrics but canâ€™t pinpoint the root cause. The issue seems tied to the container's environment and isnâ€™t easy to reproduce.

.#Challenge
You tried with standard troubleshooting methods but ran into below challenges 

1. ğ‹ğ¢ğ¦ğ¢ğ­ğğ ğ€ğœğœğğ¬ğ¬: Using `kubectl exec` only gives basic visibility into the container. You can't add new tools or make significant changes to diagnose the problem.
2. ğ“ğ«ğšğ§ğ¬ğ¢ğğ§ğ­ ğğ«ğ¨ğ›ğ¥ğğ¦ğ¬: The issue comes and goes, and traditional methods like restarting the pod donâ€™t offer enough insight.
3. ğ‘ğğ¬ğ­ğ«ğ¢ğœğ­ğğ ğ„ğ§ğ¯ğ¢ğ«ğ¨ğ§ğ¦ğğ§ğ­: Security rules and limited permissions make it tough to perform deeper diagnostics in a production setting.
4. ğ…ğ¢ğ±ğğ ğ‚ğ¨ğ§ğ­ğšğ¢ğ§ğğ« ğ’ğğ­ğ®ğ©: You canâ€™t quickly add debugging tools to the existing container without going through the hassle of building and deploying a new image.

.#Solution
As a solution you used `ğ¤ğ®ğ›ğğœğ­ğ¥ ğğğ›ğ®ğ `. It provides an effective way to tackle these challenges. Lets see how

1. ğŒğ¨ğ«ğ ğƒğğ›ğ®ğ ğ ğ¢ğ§ğ  ğğ©ğ­ğ¢ğ¨ğ§ğ¬: It lets you create a temporary copy of the pod with extra debugging tools, giving you deeper access to troubleshoot.
example :- ğ’Œğ’–ğ’ƒğ’†ğ’„ğ’•ğ’ ğ’…ğ’†ğ’ƒğ’–ğ’ˆ ğ’‚ğ’–ğ’•ğ’‰-ğ’”ğ’†ğ’“ğ’—ğ’Šğ’„ğ’†-ğ’‘ğ’ğ’… --ğ’Šğ’ğ’‚ğ’ˆğ’†=ğ’ƒğ’–ğ’”ğ’šğ’ƒğ’ğ’™ --ğ’„ğ’ğ’‘ğ’š-ğ’•ğ’=ğ’‚ğ’–ğ’•ğ’‰-ğ’”ğ’†ğ’“ğ’—ğ’Šğ’„ğ’†-ğ’…ğ’†ğ’ƒğ’–ğ’ˆ
2. ğ‹ğ¢ğ¯ğ ğ“ğ«ğ¨ğ®ğ›ğ¥ğğ¬ğ¡ğ¨ğ¨ğ­ğ¢ğ§ğ : You can attach to the pod in real time, run diagnostic commands, and investigate without changing the original pod.
example :- ğ¤ğ®ğ›ğğœğ­ğ¥ ğğğ›ğ®ğ  ğšğ®ğ­ğ¡-ğ¬ğğ«ğ¯ğ¢ğœğ-ğ©ğ¨ğ -ğ¢ğ­ --ğ¢ğ¦ğšğ ğ=ğ›ğ®ğ¬ğ²ğ›ğ¨ğ± --ğ­ğšğ«ğ ğğ­=ğšğ®ğ­ğ¡-ğœğ¨ğ§ğ­ğšğ¢ğ§ğğ«
3. ğğ¨ğ§-ğƒğ¢ğ¬ğ«ğ®ğ©ğ­ğ¢ğ¯ğ: You donâ€™t need to restart or redeploy the service, keeping production stable while you debug.
4. ğ‚ğšğ­ğœğ¡ ğˆğ§ğ­ğğ«ğ¦ğ¢ğ­ğ­ğğ§ğ­ ğˆğ¬ğ¬ğ®ğğ¬: With `kubectl debug`, you have a better chance of catching issues as they happen.

==================================================================================================
#Scenario:
A DevOps engineer is managing an e-commerce application on a Linux-based server during a major festive sale(Diwali). As traffic surges, customers report slow load times and occasional timeouts. The engineer needs to diagnose the issue fast to restore smooth operation.

.#Challenge:
- Huge volume of logs and metrics makes identifying issues time-consuming.
- Basic tools like `top` and `netstat` provide limited, real-time information.
- High traffic and fluctuating demand create sudden spikes, complicating root cause analysis.

.#Solution:
The devops engineer used nload and htop alongside atop for system-level performance analysis. Hereâ€™s how each tool helps:
1. .#nload: This tool gives a real-time view of network traffic, allowing the engineer to monitor incoming and outgoing data rates per network interface. This way, they can spot unusual spikes in network usage that might be impacting performance.
2. .#htop: Provides an interactive, real-time view of system resource usage (CPU, memory, and more) by each process. This allows quick identification of any process consuming unusually high resources.
3. .#atop: For historical analysis, atop records system performance metrics over time, allowing the engineer to pinpoint exactly when and where a slowdown occurred. With this, they can correlate network spikes with CPU/memory usage spikes or I/O bottlenecks, gaining a comprehensive view of system health across time periods.

=============================================================================================================================================================================================================================
#Scenario
You as a Devops engineer, supporting the data team in managing their Kubernetes workloads which are running on the latest Kubernetes version. The data engineering team runs a large-scale Kubernetes job with thousands of pods to process data overnight, setting a ğ›ğšğœğ¤ğ¨ğŸğŸğ‹ğ¢ğ¦ğ¢ğ­ to retry failed pods up to three times.

What is ğ›ğšğœğ¤ğ¨ğŸğŸğ‹ğ¢ğ¦ğ¢ğ­ ?
The backoffLimit in Kubernetes is a setting that limits the number of times Kubernetes will retry failed pods in a job. If a pod fails repeatedly, Kubernetes will keep restarting it until the retry count reaches the backoffLimit.

.#Challenge
You observed excessive pod restarts until reaching the backoffLimit lead to increased operating costs. This issue escalates when dealing with thousands of long-running pods across numerous nodes, as the cost of these retries quickly adds up. Now as a responsible .#Devops engineer you want to optimise this.

.#Solution
You suggested the Data team to use ğğ¨ğ ğ…ğšğ¢ğ¥ğ®ğ«ğ ğğ¨ğ¥ğ¢ğœğ², using this feature the team configures selective retries. For example, they set policies to skip retries on specific exit codes (e.g., external dependency errors) but retry on recoverable issues. This approach minimizes unnecessary retries and associated costs while ensuring efficient job processing.

====================================================================================================================
#Scenario
Imagine a DevOps engineer is managing a single Kubernetes cluster that hosts development, testing, and production microservices. Among all, the production pods must run continuously, even during high usage periods, while dev and test pods can afford some downtime.

.#Challenge
The engineer needs to make sure that production pods are the last to be evicted when resources run low, prioritizing high availability for these critical services.

.#Solution
The engineer can assign a higher .#PriorityClass to production pods. This configuration gives them a higher priority over dev and test pods. When the cluster encounters resource constraints, Kubernetes will preempt (evict) lower-priority dev and test Pods to free resources for production pods, so that they operate uninterrupted.

.#GoodtoKnow: How PriorityClass and Preemption Work ğŸ§
- .#PriorityClass: In Kubernetes, it is a resource that defines the priority of Pods. A higher value means a higher priority, which Kubernetes uses to decide which Pods should stay active during resource shortages. 
 
- .#Preemption: When a high-priority Pod cannot be scheduled due to a lack of resources, Kubernetes will attempt to preempt (evict) lower-priority Pods to make room. 

==========================================================================================================================================================
#Tip on .#AWS .#EC2 vs .#Fargate on .#ECS .#Containers 
When choosing between .#EC2 and AWS .#Fargate for running containers with ECS, itâ€™s important to evaluate your priorities

- .#Pricing: With EC2, you pay for the instances you manage, and savings depend on how well you optimize utilization , poor utilization means wastage . On the other hand, Fargate charges based on the exact CPU and memory your task uses, offering more cost-efficiency for smaller or bursty workloads, even though the per-hour cost may seem higher.

- .#Performance: EC2 offers a wide range of instance types for your specific needs (CPU, memory, network). Fargate abstracts this away, while you donâ€™t choose hardware, it automatically upgrades over time. EC2 allows for better control and faster access to the latest hardware for performance-sensitive tasks.

- .#AdministrativeOverhead: Running on EC2 requires maintaining instances, patching, and managing Docker updates. With Fargate, AWS handles all underlying infrastructure, so you can focus on building and scaling your app.

- .#DemandVariance: Fargate shines in dynamic environments where demand fluctuates or for short-lived tasks like cron jobs. It allows you to scale down to minimal resources and pay only for what you use, while EC2 is more cost-effective for consistently high-demand, large-scale workloads.

.#ProTip
For startups, fast-growing teams, or bursty workloads, Fargate reduces operational burden and maximizes agility. For high and steady demand at scale, EC2 can be optimized for cost savings and performance.

===================================================================================
When you're running .#Terraform, one of the widely used .#IAC tool , the first step is always ğ­ğğ«ğ«ğšğŸğ¨ğ«ğ¦ ğ¢ğ§ğ¢ğ­. Here is what happens behind the scenes when you run this command.
- Validates the configuration for any missing variables and syntax errors.
- Initializes any modules if needed.
- It sets up backend configuration to manage the state.
- Terraform downloads the necessary provider plugins.
- Prepares your environment to apply infrastructure changes.

ğŸ› ï¸ ğ”ğ¬ğğŸğ®ğ¥ ğ­ğğ«ğ«ğšğŸğ¨ğ«ğ¦ ğ¢ğ§ğ¢ğ­ ğğ©ğ­ğ¢ğ¨ğ§ğ¬
-ğ›ğšğœğ¤ğğ§ğ-ğœğ¨ğ§ğŸğ¢ğ : Specify a custom backend configuration.
-ğ ğğ­: Automatically download and update modules.
-ğ¢ğ§ğ©ğ®ğ­=ğ­ğ«ğ®ğ: Ask for input if necessary. If false, will error if input was required.
-ğ®ğ©ğ ğ«ğšğğ: Upgrade modules and plugins.
-ğ¥ğ¨ğœğ¤-ğ­ğ¢ğ¦ğğ¨ğ®ğ­=<ğğ®ğ«ğšğ­ğ¢ğ¨ğ§>: Override the time Terraform waits to acquire a state lock (default is 0s).
-ğ¥ğ¨ğœğ¤=ğŸğšğ¥ğ¬ğ: Disable state locking entirely.
-ğœğ¡ğğ¢ğ«=<ğ©ğšğ­ğ¡>: Switch to a different working directory before executing.

===================================================================================================
Most of the .#Devops professional working with .#Kubernetes have used ğ¤ğ®ğ›ğğœğ­ğ¥ ğ­ğ¨ğ© command to check resource usage... but do you know how the command gets its output? ğŸ¤” And what's the prerequisite for it to work in your cluster?

ğğ«ğğ«ğğªğ®ğ¢ğ¬ğ¢ğ­ğ: The ğŒğğ­ğ«ğ¢ğœğ¬ ğ’ğğ«ğ¯ğğ« must be installed and running in your cluster to retrieve these metrics!
And Here's a quick breakdown of the process
- ğ”ğ¬ğğ«: Executes kubectl top node or pod.
- ğ¤ğ®ğ›ğğœğ­ğ¥: Sends the request to the API Server.
- ğ€ğğˆ ğ’ğğ«ğ¯ğğ«: Forwards it to the Metrics Server (pre-installed in the cluster).
- ğŒğğ­ğ«ğ¢ğœğ¬ ğ’ğğ«ğ¯ğğ«: Collects data from Kubelet on each node.
- ğŠğ®ğ›ğğ¥ğğ­: Retrieves real-time CPU & memory metrics from cAdvisor.
- ğŒğğ­ğ«ğ¢ğœğ¬ ğ’ğğ«ğ¯ğğ«: Aggregates the data from all nodes and share it to API Server
- ğ€ğğˆ ğ’ğğ«ğ¯ğğ«: Forwards the resource usage to the kubectl.

===================================================================================================================
Want to know how .#AWS .#SSM agent communicates with ğ’ğ²ğ¬ğ­ğğ¦ ğŒğšğ§ğšğ ğğ« ğ’ğğ«ğ¯ğ¢ğœğ using ğ•ğğ‚ ğˆğ§ğ­ğğ«ğŸğšğœğ ğğ§ğğ©ğ¨ğ¢ğ§ğ­ğ¬.

1ï¸âƒ£ ğ‚ğšğ¥ğ¥ğ¬ ğˆğ§ğ¬ğ­ğšğ§ğœğ ğŒğğ­ğšğğšğ­ğš ğ’ğğ«ğ¯ğ¢ğœğ: The SSM agent gets the instance metadata for example AWS region.
2ï¸âƒ£ ğƒğğ’ ğ‹ğ¨ğ¨ğ¤ğ®ğ© ğŸğ¨ğ« ğ€ğğˆ ğ„ğ§ğğ©ğ¨ğ¢ğ§ğ­: The SSM agent attempts to resolve the API endpoint (e.g., ssm.<region>.amazonaws.com) via the private DNS.
3ï¸âƒ£ ğğ«ğ¢ğ¯ğšğ­ğ ğƒğğ’ ğ‘ğğ¬ğ¨ğ¥ğ¯ğğ¬ ğ­ğ¨ ğ•ğğ‚ ğ„ğ§ğğ©ğ¨ğ¢ğ§ğ­: The private DNS resolves the SSM API domain to the private IP address of the VPC interface endpointâ€™s ENI.
4ï¸âƒ£ ğ“ğ«ğšğŸğŸğ¢ğœ ğ‘ğ¨ğ®ğ­ğğ ğ­ğ¨ ğ„ğ§ğğ©ğ¨ğ¢ğ§ğ­ ğ„ğğˆ: The EC2 instance sends the API request to the private IP address of the VPC interface endpoint's ENI.
5ï¸âƒ£ ğğ«ğ¢ğ¯ğšğ­ğğ‹ğ¢ğ§ğ¤ ğ‚ğ¨ğ¦ğ¦ğ®ğ§ğ¢ğœğšğ­ğ¢ğ¨ğ§: The VPC interface endpoint forwards the request over AWS PrivateLink to the AWS SSM service.
6ï¸âƒ£ ğ’ğ’ğŒ ğ’ğğ«ğ¯ğ¢ğœğ ğğ«ğ¨ğœğğ¬ğ¬ğğ¬ ğ‘ğğªğ®ğğ¬ğ­: AWS Systems Manager processes the API request and Response Sent via PrivateLink to the VPC interface endpoint
7ï¸âƒ£ ğ‘ğğ¬ğ©ğ¨ğ§ğ¬ğ ğƒğğ¥ğ¢ğ¯ğğ«ğğ ğ­ğ¨ ğ’ğ’ğŒ ğ€ğ ğğ§ğ­: The VPC interface endpoint forwards the response to the EC2 instance, where the SSM agent receives and processes it.

=========================================================================================================================
In .#ğŠğ®ğ›ğğ«ğ§ğğ­ğğ¬ , Do you know the difference between ğ•ğ¨ğ¥ğ®ğ¦ğ ğğ¢ğ§ğğ¢ğ§ğ  and ğ•ğ¨ğ¥ğ®ğ¦ğ ğŒğ¨ğ®ğ§ğ­ğ¢ğ§ğ  ?

ğ•ğ¨ğ¥ğ®ğ¦ğ ğğ¢ğ§ğğ¢ğ§ğ 
 - The PersistentVolume (PV)represents a storage resource in the cluster.
 - The PersistentVolumeClaim (PVC)is a request for storage.
 - The binding process occurs when a PVC is associated with a PV that meets the requested storage size and other specifications.
 - This step is about reserving storage for the pod, but the actual storage is not yet attached to the pod.

ğ•ğ¨ğ¥ğ®ğ¦ğ ğŒğ¨ğ®ğ§ğ­ğ¢ğ§ğ 
 - After the binding is complete, the pod will mount the storage. 
 - The mounted PV (from the PVC) is attached to a specific directory in the podâ€™s filesystem, allowing the pod to read from or write to the PV.

 ====================================================================================================
 #Scenario
A development team is already using a Kubernetes cluster for their applications and now wants to run their CI/CD jobs on the same cluster for better scalability and resource efficiency. However, they face key .#Challenges
1. ğ’ğğœğ®ğ«ğ¢ğ­ğ² ğ‘ğ¢ğ¬ğ¤ğ¬ ğ°ğ¢ğ­ğ¡ ğŠğ®ğ›ğğ‚ğ¨ğ§ğŸğ¢ğ : They want to connect GitLab pipelines to Kubernetes without storing KubeConfig files, as it poses a security risk.
2. ğğ®ğ¢ğ¥ğğ¢ğ§ğ  ğƒğ¨ğœğ¤ğğ« ğˆğ¦ğšğ ğğ¬: With Kubernetes moving away from Docker, they need an alternative to Docker-in-Docker (DinD) to build images within the CI/CD pipeline.
3. ğŒğ®ğ¥ğ­ğ¢-ğ„ğ§ğ¯ğ¢ğ«ğ¨ğ§ğ¦ğğ§ğ­ ğƒğğ©ğ¥ğ¨ğ²ğ¦ğğ§ğ­ğ¬: They need a simplified way to deploy across dev, test, and prod using a single Helm chart, avoiding complex configurations.
4. ğ‚ğ¨ğğ ğğ®ğšğ¥ğ¢ğ­ğ² ğšğ§ğ ğ’ğğœğ®ğ«ğ¢ğ­ğ²: Automated tools for code linting and vulnerability scanning are required to maintain high standards across environments.

As a .#Devops engineer you are tasked to find an integrated solution to overcome these challenges and streamline their CI/CD process.

.#Solution
In my latest video, I showcase how to implement a complete  ğ‚ğˆ/ğ‚ğƒ pipeline with ğ†ğ¢ğ­ğ‹ğšğ› ğŠğ®ğ›ğğ«ğ§ğğ­ğğ¬ ğ‘ğ®ğ§ğ§ğğ«ğ¬, integrating top .#DevOps tools like MegaLinter, Kaniko, Trivy, Helm, and the powerful ğ†ğ¢ğ­ğ‹ğšğ› ğŠğ€ğ’ ğ€ğ ğğ§ğ­. 
ğ–ğ¡ğšğ­â€™ğ¬ ğ¢ğ§ğ¬ğ¢ğğ?
- ğƒğ¨ğ§â€™ğ­ ğ°ğšğ§ğ­ ğ­ğ¨ ğ¬ğ­ğ¨ğ«ğ ğŠğ®ğ›ğğ‚ğ¨ğ§ğŸğ¢ğ ? ğğ¨ ğ©ğ«ğ¨ğ›ğ¥ğğ¦! See how the ğ†ğ¢ğ­ğ‹ğšğ› ğŠğ€ğ’ ğ€ğ ğğ§ğ­ connects to Kubernetes clusters.
- ğ„ğŸğŸğ¨ğ«ğ­ğ¥ğğ¬ğ¬ ğƒğğ©ğ¥ğ¨ğ²ğ¦ğğ§ğ­ğ¬ across dev, test, and prod using a single Helm chart.
- ğŠğ®ğ›ğğ«ğ§ğğ­ğğ¬ ğ‘ğ®ğ§ğ§ğğ«ğ¬ running isolated CI/CD jobs in scalable Kubernetes pods.
- ğ‚ğ¨ğğ ğğ®ğšğ¥ğ¢ğ­ğ² ğ€ğ¬ğ¬ğ®ğ«ğšğ§ğœğ with MegaLinter to keep your codebase clean and compliant.
- ğ’ğğœğ®ğ«ğ¢ğ­ğ² ğ…ğ¢ğ«ğ¬ğ­ with Automated vulnerability scanning using Trivy.
- ğğ®ğ¢ğ¥ğ ğšğ§ğ ğğ«ğ¨ğ¦ğ¨ğ­ğ with ğŠğšğ§ğ¢ğ¤ğ¨

===================================================================================================================
In .#DevOps, choosing between self-hosted and SaaS-provided runners for your CI/CD workflows can significantly impact performance, cost, and maintenance. Hereâ€™s a quick summary

ğŸŒ SaaS-Provided Runners (e.g., GitHub Actions, GitLab CI)
Use when:
- You want ease of use with minimal setup and no infrastructure to manage.
- Your workload fluctuates and you need auto-scaling without any level of management.
- Youâ€™re working on small to mid-scale projects, where predictable costs (pay-as-you-go) are more feasible.
- You donâ€™t require complex or custom configurations in the runnerâ€™s environment.

Ideal for:
- Teams needing a quick and scalable setup.
- Projects with moderate resource requirements or intermittent jobs.

ğŸ› ï¸ Self-Hosted Runners
Use when:
- You need full control over the environment (custom OS, software, resources).
- High-volume pipelines or resource-heavy builds make SaaS pricing expensive.
- You have specific compliance or security requirements that SaaS runners canâ€™t meet.
- You want to avoid usage limits or need dedicated resources for continuous builds.
- You have the infrastructure and want to optimize cost-efficiency for large-scale CI/CD jobs.

Ideal for:
- Large enterprises, regulated industries, or teams with unique build environments.

===============================================================================================
#Scenario
A team of developers are utilizing github actions as CI pipeline for building their application and using github runners for it. With the rapid increase in development the team is facing issues with the delay in pipeline completion as it is taking a lot of time for building.

.#Challenge
As a .#Devops engineer you are tasked to optimize the build time by using some mechanism but for now they wonâ€™t change the runners and continue using the github provided runners only.

.#Solution
You started by checking some of the completed pipeline build logs and found that some of the dependency installations are taking insane long time as for every pipeline execution the dependencies are installed from scratch.
 As a solution you plan to implement GitHub Actions .#cache mechanism using â€˜actions/cache@v4â€™ to store dependencies, allowing future runs to reuse these assets, which significantly speeds up the pipeline. 

.#GoodToKnow
Some of the other available Pre-built Cache Actions in GitHub which requires minimum configurations. 
Below is the list of Package managers and their setup-* action for caching.
1ï¸âƒ£npm, Yarn, pnpm â€” setup-node
2ï¸âƒ£pip, pipenv, Poetry â€” setup-python
3ï¸âƒ£Gradle, Maven â€” setup-java
4ï¸âƒ£RubyGems â€” setup-ruby
5ï¸âƒ£Go go.sum â€” setup-go
6ï¸âƒ£.NET NuGet â€” setup-dotnet


======================================================================================================
As a DevOps professional , We should be familiar with the various package management systems associated with different programming languages . These are needed for building,deploying and automations.. Hereâ€™s the list of popular languages and their packages

1ï¸âƒ£ Java - JAR (Java Archive): 
 - Use Case: JAR files bundle Java class files, resources, and metadata into a single archive. This simplifies deployment, making it easier to manage Java applications and libraries across environments.

2ï¸âƒ£ .NET - NuGet: 
 - Use Case: NuGet acts as the package manager for .NET applications. Nuget packages are used for dependency management and used while building.NET apps.

3ï¸âƒ£ Python - PyPI (Python Package Index): 
 - Use Case: PyPI, accessed via the `pip` command, allows to manage Python dependencies . This is crucial for automating deployment pipelines, where consistent library versions are necessary for reliability.

4ï¸âƒ£ JavaScript - npm (Node Package Manager): 
 - Use Case: npm is used for managing JavaScript dependencies in web applications and microservices. DevOps teams leverage npm in CI/CD pipelines to ensure all necessary libraries are installed and updated during the build process.

5ï¸âƒ£ Ruby - RubyGems: 
 - Use Case: RubyGems facilitates the packaging and distribution of Ruby libraries. 

6ï¸âƒ£ Go - Go Modules: 
 - Use Case: Go Modules help manage dependencies in Go applications. They provide a structured way to define and control library versions and code reusability .

7ï¸âƒ£ PHP - Composer: 
 - Use Case: Composer is essential for managing PHP libraries, allowing DevOps teams to automate dependency installation and updates.

===========================================================================================
Being a .#Cloud and .#Devops Engineer we mostly encounter connectivity issues or service downtime. Knowing a few basic network and DNS troubleshooting techniques can help you identify and resolve the problem quickly! Here are some basics.

1ï¸âƒ£ Ping Test: 
Start with the `ping` command to check connectivity to a website or IP address. It helps determine whether your device can communicate with the server.

.#Command
ping google.com

- Success: You're connected to the network.
- Failure: There may be a network issue or the destination is unreachable.

2ï¸âƒ£ DNS Lookup:
If you can ping an IP but not a domain, there may be a DNS issue. Use `nslookup` or `dig` to test if DNS is resolving correctly.

.#Command
nslookup example.com

This will show the IP address your DNS server resolves for the domain.

3ï¸âƒ£ Check Network Configuration:
Check your network configuration using `ifconfig` (Linux/macOS) or `ipconfig` (Windows) to make sure your device has a proper IP address, gateway, and DNS settings.

.#Command
ipconfig 

4ï¸âƒ£ Telnet to Check Open Ports: 
If you're trying to access a service but it's not responding, use `telnet` to check if a specific port is open and accessible.

.#Command
telnet example.com 80

If the connection fails, the service might be down or blocked by a firewall.

5ï¸âƒ£Netstat for Active Connections:
Run netstat to view active network connections and see which ports are being used by applications. This is useful for checking open ports and connections on your device.

.#Command
netstat -an

It will display all active network connections, which can help identify potential issues.

6ï¸âƒ£ Traceroute:
Use traceroute to map the path data packets take to reach a destination, which helps diagnose where delays or blockages occur in the network.

.#Command
traceroute google.com

With these tools, you'll be well-equipped to handle basic network and DNS issues!

======================================================================================================
Here are the tools I have come to rely on for my day-to-day .#Cloud and .#Devops tasks (Not sure if there are any missing ones ğŸ˜€), with a few additional web-based ones as well which are not mentioned here.

.#Zsh: A powerful shell for efficient command-line operations, love this for ease of terminal use.
.#IntelliJ IDEA: An IDE , I mostly use this for terraform stuffs.
.#VSCode: No need of intro for well known IDE, i prefer this for docker, yaml , app codes etc.
.#kubectl: CLI tool for interacting with Kubernetes clusters.
.#Helm: Kubernetes package manager for managing and deploying apps.
.#Terraform: Infrastructure as Code tool to manage cloud resources declaratively.
.#Python: For automation, scripting, and more.
.#Homebrew: A package manager for macOS, helps me with additional tools and software installation.
.#AWS CLI: Command-line tool for managing AWS services.
.#eksctl: CLI for managing Kubernetes clusters on AWS EKS.
.#Docker Desktop: Local containerization tool for building, testing, and running apps.
.#Go: For automation, scripts.
.#Git: For interacting, collaborating with Github for code related stuff.
.#tfenv: Terraform version manager to switch between Terraform versions easily.
.#Conda: A package manager and environment manager for Python and other languages.

===================================================================================================
Helm Upgrade with Multiple Values Files. How It Works ?

hashtag#Example command 
helm upgrade <release-name> <chart-name> -f values1.yaml -f values2.yaml

When using hashtag#Helm to manage hashtag#Kubernetes applications, handling configurations with multiple `values.yaml` files is a common practice.

When you run the `helm upgrade` command with multiple `values.yaml` files, Helm merges these values in a left-to-right order. This means:

1. Last file wins: If the same key is defined in multiple files, the value from the file specified last will override earlier ones.

In our example , `values2.yaml` will override any overlapping keys in `values1.yaml`.

2. Merging objects : Helm can merge nested YAML structures, so if two files contain different keys within a nested object, Helm will combine them without overriding the entire object.

This approach gives you the flexibility to maintain base configurations (e.g., defaults) in one file and environment-specific overrides in others, making your deployment configurations scalable and manageable.

======================================================================================
Let's take a journey through time to understand the evolution of Tech and how hashtag#Kubernetes became the legend.

hashtag#Traditional Deployment 
In the early days, applications ran directly on physical servers. This led to resource allocation issues, where one application could hog resources, causing others to underperform. To resolve this, organizations would place each application on a separate physical server which is an inefficient and costly solution.

hashtag#Virtualized Deployment
With the use of virtualization, organizations could run multiple Virtual Machines (VMs) on a single physical server. Each VM operated in isolation, offering enhanced resource utilization and scalability, but still required its own operating system. This was a step forward in reducing costs and improving flexibility.

hashtag#Container Deployment 
Containers brought a revolution! Unlike VMs, containers share the host OS while maintaining isolation. They are lightweight, portable across environments, and offer unmatched scalability and efficiency. 

Entry of hashtag#Kubernetes
But there is a need to manage containers that run the apps for example scheduling, restarting etc. So wonâ€™t it be easy if someone does this for us automatically. Here Kubernetes took the opportunity to 
do container management.

==================================================================================================================
Do You Know? Windows Container Logging is Different from Linux ContainersğŸ³
Here is the key difference between Linux and Windows containers.
ğŸ§ ğ‹ğ¢ğ§ğ®ğ± ğ‚ğ¨ğ§ğ­ğšğ¢ğ§ğğ«ğ¬
By default, applications running in Linux containers output logs to STDOUT/STDERR. This allows for easily view logs using commands like 'docker logs <container_id>'.

ğŸªŸ ğ–ğ¢ğ§ğğ¨ğ°ğ¬ ğ‚ğ¨ğ§ğ­ğšğ¢ğ§ğğ«ğ¬
Windows containers keep things interesting by not exposing logs to STDOUT/STDERR by default. Instead, Windows applications typically log to Event Tracing for Windows (ETW), Event Logs, Log Files . 

This means developers and hashtag#DevOps teams often have to find some solution or else troubleshooting or monitoring windows containers will be a painful task.

hashtag#Solution
Here's a solution! There is this open-source tool called ğ‹ğ¨ğ  ğŒğ¨ğ§ğ¢ğ­ğ¨ğ«. This powerful tool bridges the gap by routing Windows application logs to STDOUT/STDERR. By integrating Log Monitor you can easily access logs similar to Linux Containers.

================================================================================================================
#Scenario:
Lets imagine there is a new CI/CD pipeline for building Docker images of microservices, pushing them to Artifactory, and deploying them via Helm on Kubernetes . The images are tagged with branch name.

For example, if the branch is `feature-X`, the image tags would be:
- `feature-Xâ€™

A developer creates a new feature branch called `feature-X` and successfully deploys it. During testing, the developer discovers that the new feature is incomplete. They push additional changes to the same branch, and the pipeline runs successfully. However, after the deployment, the developer notices that the updated feature is not reflected in the microservice.

hashtag#Challenge:
As a DevOps engineer, you are tasked with troubleshooting the issue. Despite the pipeline showing a successful build and deployment, the service still runs the old version of the code. 

hashtag#Troubleshooting Steps:
- You started by checking the CI/CD pipeline logs and verified that the image is build and pushed and also Helm is reporting a successful deployment.

- Next, you reviewed the Helm chart configuration and found that the hashtag#imagePullPolicy is set to hashtag#IfNotPresent. This policy caused Kubernetes to skip pulling the latest image.

hashtag#Solutions
Two Possible Solutions to choose from. 
1. Change the Image Pull Policy: 
 Update the Helm chart to set the hashtag#imagePullPolicy to hashtag#Always. This ensures that Kubernetes always pulls the latest image from the Artifactory during deployments.
 ```yaml
 image:
 pullPolicy: Always
 ```
2. Update the CICD pipeline to tag the images for every pipeline run, for example you can add pipeline ID to the tag. This will force Kubernetes to pull the updated image every time deployment is made via pipeline.

===============================================================================================================
#Scenario
As a DevOps engineer you and your team are setting up a DR kubernetes cluster and deploying multiple microservices in parallel . The team noticed that deployments are slowing down as it is taking delays in getting pods ready, inspite of having a large bandwidth and system capacity.

hashtag#Challenge
You need to speed up the process so that the deployments get ready quickly without overwhelming the container runtime or affecting the stability of the cluster and only if the runtime supports.

hashtag#Solution
You resolve this challenge by updating the kubelet configuration by setting hashtag#serializeImagePulls to false. This enables parallel image pulls, allowing multiple images to be fetched simultaneously, significantly speeding up deployments across the cluster. And additionally setting the hashtag#maxParallelImagePulls to a value comes handy while controlling the parallel image pulls.

hashtag#GoodtoKnow : The kubelet never pulls multiple images in parallel on behalf of one Pod. For example an init container and the main container wonâ€™t be pulled in parallel. It works only for multiple pods in a single node.

========================================================================================================================
#Scenario:
Imagine you are managing infrastructure across multiple AWS accounts with VPC hashtag#peering. Everything works fine, but now youâ€™re ready to scale. Migrating to AWS Transit Gateway seems like the best option to streamline connections across your VPCs. But you have many cross VPC Security Group referencing which you are worried about.

hashtag#Challenge:
Managing security groups across accounts and VPCs seems like a major challenge, especially when it comes to referencing security rules between them. You donâ€™t want to manually update the IPâ€™s and CIDR ranges for the referencing across VPC security groups.

hashtag#Solution:
AWS has introduced a new Security Group Referencing feature in hashtag#TransitGateway! ğŸ‰ Now, instead of manually configuring IPs or CIDRs for every VPC, you can hashtag#reference security groups across VPCs attached to the same Transit Gateway in the same region. This simplifies security management and makes your migration to Transit Gateway seamless. 

=========================================================================


